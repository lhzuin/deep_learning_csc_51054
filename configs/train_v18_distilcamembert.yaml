experiment_name: "train_v18_distilcamembert" 
log: true
seed: 42
acc_epsilon: 1e-3
val_type: "author_based"


train_stages:
  - name: stage1_text_base
    epochs: 2
    warmup_fraction: 0.02
    lr_scheduler: constant_with_warmup
    groups: ["head", "tweet_proj"]   # crucial: NO LoRA, NO desc, NO metadata

  - name: stage2_text_lora_tweet
    epochs: 2
    warmup_fraction: 0.02
    lr_scheduler: cosine
    groups: ["head", "tweet_proj", "text_lora_tweet"]

  - name: stage3_full
    epochs: 4
    warmup_fraction: 0.02
    lr_scheduler: cosine
    groups: ["head", "tweet_proj", "desc_proj",
             "text_lora_tweet", "text_lora_desc",
             "source_emb", "num_proj"]


use_warmup: true        

checkpoint_save_path: "influencer_lora.pt"
checkpoint_load_path: ${hydra:runtime.cwd}/checkpoints/influencer_lora.pt # "/Users/luiszuin/Desktop/Polytechnique/3AP1/DeepLearning/Kaggle2025/checkpoints/influencer_lora.pt"
predict_path: ${hydra:runtime.cwd}/../kaggle_test.jsonl
output_csv: ${hydra:runtime.cwd}/submission.csv

early_stopping:
  patience: 2
  min_epochs: 2

datamodule:
  _target_: data.datamodule_tweets.TweetsDataModule
  train_path: ${hydra:runtime.cwd}/../train.jsonl
  batch_size: 32 # maybe reduce?
  num_workers: 2
  val_size: 0.1
  random_state: ${seed}

model:
  instance:
    _target_: models.influencer_distilcamembert.InfluencerDistilCamembert
    n_source_buckets: 999   # placeholder; weâ€™ll override at runtime
    source_emb_dim: 16
    num_proj_dim: 16
    head_hidden_dim: 256
    head_dropout: 0.15 #Increase?
    max_len_tweet: 128
    max_len_desc: 96
    meta_dropout: 0.5 

optim:
  _target_: torch.optim.AdamW
  lr_class: 3e-4   # head + projections
  lr_lora:  4e-5   # LoRA adapters
  lr_meta_proj: 3e-4 # Meta projections
  lr: 2e-4         # base lr (unused directly; AdamW needs it)
  weight_decay: 0.01 # Decreased from 0.01

loss_fn:
  _target_: torch.nn.NLLLoss