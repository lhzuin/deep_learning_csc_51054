experiment_name: "tweets_influencer_v1"
log: true
epochs_stage1: 2
epochs_stage2: 4
use_warmup: true
warmup_fraction: 0.06
checkpoint_path: "influencer_lora.pt"
#checkpoint_path: "/Users/luiszuin/Desktop/Polytechnique/3AP1/DeepLearning/Kaggle2025/checkpoints/influencer_lora.pt"
predict_path: ${hydra:runtime.cwd}/kaggle_test.jsonl
output_csv: ${hydra:runtime.cwd}/submission.csv

early_stopping:
  patience: 3
  min_epochs: 3

datamodule:
  _target_: data.datamodule_tweets.TweetsDataModule
  train_path: ${hydra:runtime.cwd}/train.jsonl
  batch_size: 32
  num_workers: 2

model:
  instance:
    _target_: models.influencer_distilbert.InfluencerDistilBert
    n_source_buckets: 999   # placeholder; weâ€™ll override at runtime
    source_emb_dim: 16
    num_proj_dim: 16
    head_hidden_dim: 256
    head_dropout: 0.15
    max_len_tweet: 128
    max_len_desc: 96

optim:
  _target_: torch.optim.AdamW
  lr_class: 2e-4   # head + projections
  lr_lora:  2e-4   # LoRA adapters
  lr: 2e-4         # base lr (unused directly; AdamW needs it)
  weight_decay: 0.01

loss_fn:
  _target_: torch.nn.NLLLoss