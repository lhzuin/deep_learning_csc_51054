{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abe1e53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.stats import (\n",
    "    pointbiserialr, spearmanr, mannwhitneyu,\n",
    "    chi2_contingency, fisher_exact\n",
    ")\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1. Small utilities\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "def _safe_series(df: pd.DataFrame, name: str) -> bool:\n",
    "    \"\"\"Check if column exists in df.\"\"\"\n",
    "    return name in df.columns\n",
    "\n",
    "\n",
    "def _group_topk(s: pd.Series, k: int) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Keep top-k most frequent categories, map the rest to \"Other\".\n",
    "    Works on any hashable dtype (string, int, etc.).\n",
    "    \"\"\"\n",
    "    vc = s.value_counts(dropna=False)\n",
    "    keep = set(vc.head(k).index)\n",
    "    return s.where(s.isin(keep), other=\"Other\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2. Dataset summary helpers\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "def summarize_dataset(df: pd.DataFrame, label_col: str = \"label\") -> None:\n",
    "    \"\"\"\n",
    "    Print basic info about the dataset: shape, label distribution,\n",
    "    missingness and some type/cardinality info.\n",
    "    \"\"\"\n",
    "    print(\"=== DATASET SUMMARY ===\")\n",
    "    print(\"Shape:\", df.shape)\n",
    "\n",
    "    if label_col in df.columns:\n",
    "        y = df[label_col]\n",
    "        print(\"\\nLabel distribution:\")\n",
    "        print(y.value_counts(dropna=False))\n",
    "        print(\"Pos rate:\", y.mean())\n",
    "    else:\n",
    "        print(f\"\\n[Warning] Label column '{label_col}' not found in df.\")\n",
    "\n",
    "    print(\"\\nTop 30 columns by missingness:\")\n",
    "    miss = df.isna().mean().sort_values(ascending=False)\n",
    "    print(miss.head(30))\n",
    "\n",
    "    print(\"\\nColumn types and cardinality (first 40):\")\n",
    "    info_rows = []\n",
    "    for col in df.columns[:40]:\n",
    "        ser = df[col]\n",
    "        info_rows.append({\n",
    "            \"col\": col,\n",
    "            \"dtype\": str(ser.dtype),\n",
    "            \"n_unique\": ser.nunique(dropna=True),\n",
    "            \"missing_frac\": ser.isna().mean(),\n",
    "        })\n",
    "    info_df = pd.DataFrame(info_rows)\n",
    "    display(info_df)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3. Automatic feature detection (numeric / categorical)\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "def auto_detect_features(\n",
    "    df: pd.DataFrame,\n",
    "    label_col: str = \"label\",\n",
    "    min_non_na_frac: float = 0.3,\n",
    "    max_cat_cardinality: int = 50,\n",
    "    verbose: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Automatically propose numeric and categorical features based on:\n",
    "      - dtype\n",
    "      - fraction of non-missing values\n",
    "      - cardinality for categoricals\n",
    "\n",
    "    Returns:\n",
    "      numeric_feats, categorical_feats\n",
    "    \"\"\"\n",
    "    if label_col not in df.columns:\n",
    "        raise ValueError(f\"Label column '{label_col}' not in df.\")\n",
    "\n",
    "    numeric_feats = []\n",
    "    categorical_feats = []\n",
    "\n",
    "    n = len(df)\n",
    "    for col in df.columns:\n",
    "        if col == label_col:\n",
    "            continue\n",
    "\n",
    "        ser = df[col]\n",
    "        non_na_frac = 1.0 - ser.isna().mean()\n",
    "        if non_na_frac < min_non_na_frac:\n",
    "            # too sparse\n",
    "            continue\n",
    "\n",
    "        # Identify numeric/light bool columns\n",
    "        if pd.api.types.is_numeric_dtype(ser) or pd.api.types.is_bool_dtype(ser):\n",
    "            numeric_feats.append(col)\n",
    "        else:\n",
    "            # treat as categorical if not too many unique values\n",
    "            nunique = ser.nunique(dropna=True)\n",
    "            if 1 < nunique <= max_cat_cardinality:\n",
    "                categorical_feats.append(col)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"=== AUTO-DETECTED FEATURES ===\")\n",
    "        print(f\"Numeric ({len(numeric_feats)}):\", numeric_feats[:20],\n",
    "              \"...\" if len(numeric_feats) > 20 else \"\")\n",
    "        print(f\"Categorical ({len(categorical_feats)}):\", categorical_feats[:20],\n",
    "              \"...\" if len(categorical_feats) > 20 else \"\")\n",
    "\n",
    "    return numeric_feats, categorical_feats\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 4. Main analysis function (extended version of yours)\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "def analyze_influencer_correlations(\n",
    "    df: pd.DataFrame,\n",
    "    label_col: str = \"label\",\n",
    "    numeric_feats=None,\n",
    "    categorical_feats=None,\n",
    "    topk_map=None,\n",
    "    auto_topk_default: int = 20,\n",
    "):\n",
    "    \"\"\"\n",
    "    Run univariate association analysis between each feature and the binary label:\n",
    "      - For numeric features:\n",
    "          * Means/std by class\n",
    "          * Point-biserial r + p\n",
    "          * Spearman rho + p\n",
    "          * Mann-Whitney U + p\n",
    "          * 1-feature logistic coef p-value\n",
    "          * AUC (using raw feature as score)\n",
    "      - For categorical features:\n",
    "          * Chi-square (or Fisher for 2x2)\n",
    "          * CramÃ©r's V\n",
    "          * Top levels by positive rate\n",
    "\n",
    "    If numeric_feats / categorical_feats are None, they are auto-detected.\n",
    "    topk_map: dict col_name -> K for _group_topk, else auto_topk_default.\n",
    "    \"\"\"\n",
    "    assert label_col in df.columns, f\"'{label_col}' not in df\"\n",
    "\n",
    "    # Ensure binary ints\n",
    "    y = df[label_col].astype(int)\n",
    "\n",
    "    # Feature lists: auto-detect if not provided\n",
    "    if numeric_feats is None or categorical_feats is None:\n",
    "        auto_num, auto_cat = auto_detect_features(\n",
    "            df, label_col=label_col, verbose=False\n",
    "        )\n",
    "        if numeric_feats is None:\n",
    "            numeric_feats = auto_num\n",
    "        if categorical_feats is None:\n",
    "            categorical_feats = auto_cat\n",
    "\n",
    "    if topk_map is None:\n",
    "        topk_map = {}\n",
    "\n",
    "    # ---------- NUMERIC FEATURES ----------\n",
    "    num_rows = []\n",
    "    for feat in numeric_feats:\n",
    "        if not _safe_series(df, feat):\n",
    "            continue\n",
    "\n",
    "        x = df[feat]\n",
    "        # drop missing and non-finite\n",
    "        mask = x.notna() & np.isfinite(x) & y.notna()\n",
    "        x = x[mask].astype(float)\n",
    "        y_ = y[mask].astype(int)\n",
    "\n",
    "        if len(x) < 30 or y_.nunique() < 2:\n",
    "            continue\n",
    "\n",
    "        g0 = x[y_ == 0]\n",
    "        g1 = x[y_ == 1]\n",
    "        mean0, mean1 = g0.mean(), g1.mean()\n",
    "        std0, std1   = g0.std(ddof=1), g1.std(ddof=1)\n",
    "\n",
    "        # Correlations\n",
    "        try:\n",
    "            pb_r, pb_p = pointbiserialr(y_, x)\n",
    "        except Exception:\n",
    "            pb_r, pb_p = np.nan, np.nan\n",
    "\n",
    "        try:\n",
    "            sp_rho, sp_p = spearmanr(y_, x)\n",
    "        except Exception:\n",
    "            sp_rho, sp_p = np.nan, np.nan\n",
    "\n",
    "        # Mann-Whitney\n",
    "        try:\n",
    "            mw_u, mw_p = mannwhitneyu(g0, g1, alternative=\"two-sided\")\n",
    "        except Exception:\n",
    "            mw_u, mw_p = np.nan, np.nan\n",
    "\n",
    "        # 1-feature logistic + AUC\n",
    "        try:\n",
    "            X = sm.add_constant(x.values, has_constant=\"add\")\n",
    "            model = sm.Logit(y_.values, X, missing=\"drop\").fit(disp=False)\n",
    "            logit_p = model.pvalues[1] if len(model.pvalues) > 1 else np.nan\n",
    "            auc = roc_auc_score(y_, x)\n",
    "        except Exception:\n",
    "            logit_p, auc = np.nan, np.nan\n",
    "\n",
    "        num_rows.append({\n",
    "            \"feature\": feat,\n",
    "            \"n\": int(len(x)),\n",
    "            \"mean_0\": mean0, \"mean_1\": mean1,\n",
    "            \"std_0\": std0,   \"std_1\": std1,\n",
    "            \"pointbiserial_r\": pb_r, \"pointbiserial_p\": pb_p,\n",
    "            \"spearman_rho\": sp_rho,  \"spearman_p\": sp_p,\n",
    "            \"mannwhitney_U\": mw_u,   \"mannwhitney_p\": mw_p,\n",
    "            \"logit_p\": logit_p,\n",
    "            \"auc_univariate\": auc,\n",
    "        })\n",
    "\n",
    "    numeric_results = pd.DataFrame(num_rows)\n",
    "    if len(numeric_results) > 0:\n",
    "        numeric_results = numeric_results.sort_values(\n",
    "            [\"logit_p\", \"pointbiserial_p\", \"mannwhitney_p\"],\n",
    "            na_position=\"last\"\n",
    "        )\n",
    "\n",
    "    # ---------- CATEGORICAL FEATURES ----------\n",
    "    cat_rows = []\n",
    "    for feat in categorical_feats:\n",
    "        if not _safe_series(df, feat):\n",
    "            continue\n",
    "\n",
    "        s = df[feat]\n",
    "\n",
    "        # Group high-cardinality\n",
    "        k = topk_map.get(feat, auto_topk_default)\n",
    "        s = s.astype(object).fillna(\"Unknown\")\n",
    "        if s.nunique(dropna=False) > k:\n",
    "            s = _group_topk(s, k)\n",
    "\n",
    "        # Drop missing label / feature\n",
    "        mask = y.notna() & s.notna()\n",
    "        s = s[mask]\n",
    "        y_ = y[mask].astype(int)\n",
    "        if len(s) < 30 or y_.nunique() < 2:\n",
    "            continue\n",
    "\n",
    "        ct = pd.crosstab(y_, s)\n",
    "        if ct.shape[1] < 2:  # only one category after grouping\n",
    "            continue\n",
    "\n",
    "        n = ct.values.sum()\n",
    "        r, c = ct.shape\n",
    "\n",
    "        if r == 2 and c == 2:\n",
    "            odds, p_fisher = fisher_exact(ct.values)\n",
    "            chi2, p_chi2, dof, _ = chi2_contingency(ct, correction=False)\n",
    "            cramers_v = np.sqrt((chi2 / n) / (min(r - 1, c - 1)))\n",
    "            p_value = p_fisher\n",
    "            test_name = \"Fisher (2x2) + Chi2\"\n",
    "        else:\n",
    "            chi2, p_value, dof, _ = chi2_contingency(ct, correction=False)\n",
    "            cramers_v = np.sqrt((chi2 / n) / (min(r - 1, c - 1)))\n",
    "            test_name = \"Chi-square\"\n",
    "\n",
    "        # Positive rate by category\n",
    "        pos_rates = (ct.loc[1] / ct.sum(axis=0)).sort_values(ascending=False)\n",
    "\n",
    "        cat_rows.append({\n",
    "            \"feature\": feat,\n",
    "            \"k_levels\": c,\n",
    "            \"n\": int(n),\n",
    "            \"test\": test_name,\n",
    "            \"chi2\": chi2,\n",
    "            \"df\": dof,\n",
    "            \"p_value\": p_value,\n",
    "            \"cramers_v\": cramers_v,\n",
    "            \"top_levels_by_rate\": pos_rates.head(5).round(3).to_dict(),\n",
    "        })\n",
    "\n",
    "    categorical_results = pd.DataFrame(cat_rows)\n",
    "    if len(categorical_results) > 0:\n",
    "        categorical_results = categorical_results.sort_values(\n",
    "            [\"p_value\"], na_position=\"last\"\n",
    "        )\n",
    "\n",
    "    return numeric_results, categorical_results\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 5. Convenience wrapper to run everything on a df\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "def full_influencer_feature_analysis(\n",
    "    df: pd.DataFrame,\n",
    "    label_col: str = \"label\",\n",
    "    numeric_override=None,\n",
    "    categorical_override=None,\n",
    "    topk_map=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    High-level function:\n",
    "      1. Summarizes dataset.\n",
    "      2. Auto-detects feature types (unless overridden).\n",
    "      3. Runs univariate analysis.\n",
    "      4. Displays top features.\n",
    "\n",
    "    Returns:\n",
    "      numeric_results, categorical_results\n",
    "    \"\"\"\n",
    "    summarize_dataset(df, label_col=label_col)\n",
    "\n",
    "    if numeric_override is not None or categorical_override is not None:\n",
    "        numeric_feats = numeric_override\n",
    "        categorical_feats = categorical_override\n",
    "    else:\n",
    "        numeric_feats, categorical_feats = auto_detect_features(df, label_col=label_col)\n",
    "\n",
    "    print(\"\\n=== RUNNING UNIVARIATE ANALYSIS ===\")\n",
    "    num_res, cat_res = analyze_influencer_correlations(\n",
    "        df,\n",
    "        label_col=label_col,\n",
    "        numeric_feats=numeric_feats,\n",
    "        categorical_feats=categorical_feats,\n",
    "        topk_map=topk_map,\n",
    "    )\n",
    "\n",
    "    print(\"\\n=== NUMERIC FEATURES (sorted by significance) ===\")\n",
    "    display(num_res.head(50))\n",
    "\n",
    "    print(\"\\n=== CATEGORICAL / BOOLEAN FEATURES (sorted by p-value) ===\")\n",
    "    display(cat_res.head(50))\n",
    "\n",
    "    return num_res, cat_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b54dcd75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_191494/1586771364.py:107: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[\"quoted_is_sensitive\"] = df[\"quoted_status.possibly_sensitive\"].fillna(False)\n",
      "/tmp/ipykernel_191494/1586771364.py:111: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[\"quoted_user_verified\"] = df[\"quoted_status.user.verified\"].fillna(False)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, ast\n",
    "\n",
    "def parse_tweets(path, expect_label=True):\n",
    "    # Load & flatten\n",
    "    df = pd.read_json(path, lines=True)\n",
    "    df = pd.json_normalize(df.to_dict(orient=\"records\"), sep=\".\")\n",
    "\n",
    "    # Ensure expected nested columns exist\n",
    "    for col in [\n",
    "        \"text\", \"extended_tweet.full_text\", \"source\",\n",
    "        \"entities.hashtags\", \"entities.user_mentions\", \"entities.urls\",\n",
    "        \"extended_entities.media\",\n",
    "    ]:\n",
    "        if col not in df.columns:\n",
    "            df[col] = np.nan\n",
    "\n",
    "    # Full text (vectorized, avoids apply/axis=1)\n",
    "    df[\"full_text\"] = df[\"extended_tweet.full_text\"].fillna(df[\"text\"]).fillna(\"\")\n",
    "\n",
    "    # Engagement (create if missing)\n",
    "    for col in [\"retweet_count\", \"favorite_count\", \"reply_count\", \"quote_count\"]:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0\n",
    "\n",
    "    # Safe length for list-like fields (sometimes lists, sometimes stringified)\n",
    "    def safe_len(x):\n",
    "        if isinstance(x, list):\n",
    "            return len(x)\n",
    "        if isinstance(x, str):\n",
    "            try:\n",
    "                v = ast.literal_eval(x)\n",
    "                return len(v) if isinstance(v, (list, tuple)) else 1\n",
    "            except Exception:\n",
    "                return 0\n",
    "        return 0\n",
    "\n",
    "    df[\"n_hashtags\"] = df[\"entities.hashtags\"].apply(safe_len)\n",
    "    df[\"n_mentions\"] = df[\"entities.user_mentions\"].apply(safe_len)\n",
    "    df[\"n_urls\"]     = df[\"entities.urls\"].apply(safe_len)\n",
    "\n",
    "    # Media presence\n",
    "    df[\"has_media\"] = df[\"extended_entities.media\"].apply(lambda x: safe_len(x) > 0)\n",
    "\n",
    "    # Source app (extract readable name from HTML anchor)\n",
    "    def extract_source(x):\n",
    "        if not isinstance(x, str):\n",
    "            return \"Unknown\"\n",
    "        m = re.search(r'>([^<]+)<', x)\n",
    "        return m.group(1) if m else x\n",
    "\n",
    "    df[\"source_app\"] = df[\"source\"].apply(extract_source)\n",
    "\n",
    "    # User fields (create if missing)\n",
    "    for col in [\n",
    "        \"user.description\", \"user.location\",\n",
    "        \"user.favourites_count\", \"user.statuses_count\", \"user.listed_count\",\n",
    "        \"user.default_profile\", \"user.default_profile_image\",\n",
    "        \"user.geo_enabled\", \"user.protected\",\n",
    "        \"user.lang\", \"user.time_zone\",\n",
    "        \"user.created_at\",\n",
    "    ]:\n",
    "        if col not in df.columns:\n",
    "            df[col] = np.nan\n",
    "    df[\"user.description\"] = df[\"user.description\"].fillna(\"\")\n",
    "\n",
    "    # --- simple structural flags ---\n",
    "    df[\"is_reply\"] = (\n",
    "        df[\"in_reply_to_status_id\"].notna() |\n",
    "        df[\"in_reply_to_user_id\"].notna()\n",
    "    )\n",
    "\n",
    "    # approximate retweet flag (no retweeted_status in the schema you pasted)\n",
    "    df[\"is_retweet\"] = df[\"text\"].fillna(\"\").str.startswith(\"RT @\")\n",
    "\n",
    "    if \"is_quote_status\" not in df.columns:\n",
    "        df[\"is_quote_status\"] = False\n",
    "    df[\"is_quote_status\"] = df[\"is_quote_status\"].fillna(False)\n",
    "\n",
    "    if \"possibly_sensitive\" not in df.columns:\n",
    "        df[\"possibly_sensitive\"] = False\n",
    "    df[\"possibly_sensitive\"] = df[\"possibly_sensitive\"].fillna(False)\n",
    "\n",
    "    # Place / geo\n",
    "    for col in [\n",
    "        \"place.country_code\", \"place.place_type\", \"place.full_name\", \"place.id\"\n",
    "    ]:\n",
    "        if col not in df.columns:\n",
    "            df[col] = np.nan\n",
    "\n",
    "    df[\"has_place\"] = df[\"place.id\"].notna()\n",
    "\n",
    "    # quoted-status flags (minimal)\n",
    "    if \"quoted_status_id\" not in df.columns:\n",
    "        df[\"quoted_status_id\"] = np.nan\n",
    "    df[\"has_quoted_status\"] = df[\"quoted_status_id\"].notna()\n",
    "\n",
    "    if \"quoted_status.extended_entities.media\" not in df.columns:\n",
    "        df[\"quoted_status.extended_entities.media\"] = np.nan\n",
    "    df[\"quoted_has_media\"] = df[\"quoted_status.extended_entities.media\"].apply(\n",
    "        lambda x: safe_len(x) > 0\n",
    "    )\n",
    "\n",
    "    if \"quoted_status.possibly_sensitive\" not in df.columns:\n",
    "        df[\"quoted_status.possibly_sensitive\"] = np.nan\n",
    "    df[\"quoted_is_sensitive\"] = df[\"quoted_status.possibly_sensitive\"].fillna(False)\n",
    "\n",
    "    if \"quoted_status.user.verified\" not in df.columns:\n",
    "        df[\"quoted_status.user.verified\"] = np.nan\n",
    "    df[\"quoted_user_verified\"] = df[\"quoted_status.user.verified\"].fillna(False)\n",
    "\n",
    "    # --- simple text length features ---\n",
    "    df[\"tweet_len_chars\"] = df[\"full_text\"].fillna(\"\").str.len()\n",
    "    df[\"tweet_len_words\"] = df[\"full_text\"].fillna(\"\").str.split().str.len()\n",
    "\n",
    "    # Keep relevant columns (only those that exist)\n",
    "    # (you can still engineer logs/clips later in make_transformations)\n",
    "    keep_cols = [\n",
    "        # IDs / bookkeeping\n",
    "        \"id_str\",\n",
    "        \"challenge_id\",          # if present\n",
    "        # text\n",
    "        \"full_text\",\n",
    "        \"user.description\",\n",
    "        # counts + structure\n",
    "        \"n_hashtags\", \"n_mentions\", \"n_urls\", \"has_media\",\n",
    "        \"tweet_len_chars\", \"tweet_len_words\",\n",
    "        \"retweet_count\", \"favorite_count\", \"reply_count\", \"quote_count\",\n",
    "        # interaction type\n",
    "        \"is_reply\", \"is_retweet\", \"is_quote_status\",\n",
    "        \"possibly_sensitive\",\n",
    "        # source\n",
    "        \"source_app\",\n",
    "        # user info\n",
    "        \"user.location\",\n",
    "        \"user.favourites_count\",\n",
    "        \"user.statuses_count\",\n",
    "        \"user.listed_count\",\n",
    "        \"user.default_profile\",\n",
    "        \"user.default_profile_image\",\n",
    "        \"user.geo_enabled\",\n",
    "        \"user.protected\",\n",
    "        \"user.lang\",\n",
    "        \"user.time_zone\",\n",
    "        \"user.created_at\",\n",
    "        # time / place\n",
    "        \"created_at\",\n",
    "        \"timestamp_ms\",\n",
    "        \"place.country_code\",\n",
    "        \"place.place_type\",\n",
    "        \"place.full_name\",\n",
    "        \"has_place\",\n",
    "        # quoted status summary\n",
    "        \"has_quoted_status\",\n",
    "        \"quoted_has_media\",\n",
    "        \"quoted_is_sensitive\",\n",
    "        \"quoted_user_verified\",\n",
    "    ]\n",
    "    existing = [c for c in keep_cols if c in df.columns]\n",
    "    out = df[existing].copy()\n",
    "\n",
    "    # Attach label if expected and available\n",
    "    if expect_label and \"label\" in df.columns:\n",
    "        out[\"label\"] = df[\"label\"]\n",
    "    elif expect_label and \"label\" not in df.columns:\n",
    "        print(\"Warning: 'label' not found in this file; returning features only.\")\n",
    "\n",
    "    missing = sorted(set(keep_cols) - set(existing))\n",
    "    if missing:\n",
    "        print(\"Note: some expected columns not present in raw json:\", missing)\n",
    "\n",
    "    return out\n",
    "\n",
    "# Usage\n",
    "train_clean = parse_tweets(\"../../train.jsonl\", expect_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25c8c11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATASET SUMMARY ===\n",
      "Shape: (154914, 41)\n",
      "\n",
      "Label distribution:\n",
      "label\n",
      "0    82674\n",
      "1    72240\n",
      "Name: count, dtype: int64\n",
      "Pos rate: 0.46632325031953215\n",
      "\n",
      "Top 30 columns by missingness:\n",
      "user.lang                     1.000000\n",
      "user.time_zone                1.000000\n",
      "place.full_name               0.981396\n",
      "place.place_type              0.981396\n",
      "place.country_code            0.981396\n",
      "user.location                 0.341635\n",
      "id_str                        0.000000\n",
      "user.created_at               0.000000\n",
      "user.default_profile          0.000000\n",
      "user.default_profile_image    0.000000\n",
      "user.geo_enabled              0.000000\n",
      "user.protected                0.000000\n",
      "timestamp_ms                  0.000000\n",
      "created_at                    0.000000\n",
      "user.statuses_count           0.000000\n",
      "has_place                     0.000000\n",
      "has_quoted_status             0.000000\n",
      "quoted_has_media              0.000000\n",
      "quoted_is_sensitive           0.000000\n",
      "quoted_user_verified          0.000000\n",
      "user.listed_count             0.000000\n",
      "user.favourites_count         0.000000\n",
      "challenge_id                  0.000000\n",
      "tweet_len_words               0.000000\n",
      "full_text                     0.000000\n",
      "user.description              0.000000\n",
      "n_hashtags                    0.000000\n",
      "n_mentions                    0.000000\n",
      "n_urls                        0.000000\n",
      "has_media                     0.000000\n",
      "dtype: float64\n",
      "\n",
      "Column types and cardinality (first 40):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col</th>\n",
       "      <th>dtype</th>\n",
       "      <th>n_unique</th>\n",
       "      <th>missing_frac</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_str</td>\n",
       "      <td>int64</td>\n",
       "      <td>154812</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>challenge_id</td>\n",
       "      <td>int64</td>\n",
       "      <td>154914</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>full_text</td>\n",
       "      <td>object</td>\n",
       "      <td>154091</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>user.description</td>\n",
       "      <td>object</td>\n",
       "      <td>41235</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>n_hashtags</td>\n",
       "      <td>int64</td>\n",
       "      <td>15</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>n_mentions</td>\n",
       "      <td>int64</td>\n",
       "      <td>12</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>n_urls</td>\n",
       "      <td>int64</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>has_media</td>\n",
       "      <td>bool</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tweet_len_chars</td>\n",
       "      <td>int64</td>\n",
       "      <td>496</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tweet_len_words</td>\n",
       "      <td>int64</td>\n",
       "      <td>79</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>retweet_count</td>\n",
       "      <td>int64</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>favorite_count</td>\n",
       "      <td>int64</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>reply_count</td>\n",
       "      <td>int64</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>quote_count</td>\n",
       "      <td>int64</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>is_reply</td>\n",
       "      <td>bool</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>is_retweet</td>\n",
       "      <td>bool</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>is_quote_status</td>\n",
       "      <td>bool</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>possibly_sensitive</td>\n",
       "      <td>object</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>source_app</td>\n",
       "      <td>object</td>\n",
       "      <td>408</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>user.location</td>\n",
       "      <td>object</td>\n",
       "      <td>10974</td>\n",
       "      <td>0.341635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>user.favourites_count</td>\n",
       "      <td>int64</td>\n",
       "      <td>42669</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>user.statuses_count</td>\n",
       "      <td>int64</td>\n",
       "      <td>54804</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>user.listed_count</td>\n",
       "      <td>int64</td>\n",
       "      <td>1839</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>user.default_profile</td>\n",
       "      <td>bool</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>user.default_profile_image</td>\n",
       "      <td>bool</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>user.geo_enabled</td>\n",
       "      <td>bool</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>user.protected</td>\n",
       "      <td>bool</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>user.lang</td>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>user.time_zone</td>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>user.created_at</td>\n",
       "      <td>object</td>\n",
       "      <td>30696</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>created_at</td>\n",
       "      <td>datetime64[ns, UTC]</td>\n",
       "      <td>153004</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>timestamp_ms</td>\n",
       "      <td>datetime64[ns]</td>\n",
       "      <td>154810</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>place.country_code</td>\n",
       "      <td>object</td>\n",
       "      <td>63</td>\n",
       "      <td>0.981396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>place.place_type</td>\n",
       "      <td>object</td>\n",
       "      <td>4</td>\n",
       "      <td>0.981396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>place.full_name</td>\n",
       "      <td>object</td>\n",
       "      <td>741</td>\n",
       "      <td>0.981396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>has_place</td>\n",
       "      <td>bool</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>has_quoted_status</td>\n",
       "      <td>bool</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>quoted_has_media</td>\n",
       "      <td>bool</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>quoted_is_sensitive</td>\n",
       "      <td>bool</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>quoted_user_verified</td>\n",
       "      <td>bool</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           col                dtype  n_unique  missing_frac\n",
       "0                       id_str                int64    154812      0.000000\n",
       "1                 challenge_id                int64    154914      0.000000\n",
       "2                    full_text               object    154091      0.000000\n",
       "3             user.description               object     41235      0.000000\n",
       "4                   n_hashtags                int64        15      0.000000\n",
       "5                   n_mentions                int64        12      0.000000\n",
       "6                       n_urls                int64         5      0.000000\n",
       "7                    has_media                 bool         2      0.000000\n",
       "8              tweet_len_chars                int64       496      0.000000\n",
       "9              tweet_len_words                int64        79      0.000000\n",
       "10               retweet_count                int64         1      0.000000\n",
       "11              favorite_count                int64         1      0.000000\n",
       "12                 reply_count                int64         1      0.000000\n",
       "13                 quote_count                int64         1      0.000000\n",
       "14                    is_reply                 bool         2      0.000000\n",
       "15                  is_retweet                 bool         2      0.000000\n",
       "16             is_quote_status                 bool         2      0.000000\n",
       "17          possibly_sensitive               object         2      0.000000\n",
       "18                  source_app               object       408      0.000000\n",
       "19               user.location               object     10974      0.341635\n",
       "20       user.favourites_count                int64     42669      0.000000\n",
       "21         user.statuses_count                int64     54804      0.000000\n",
       "22           user.listed_count                int64      1839      0.000000\n",
       "23        user.default_profile                 bool         2      0.000000\n",
       "24  user.default_profile_image                 bool         1      0.000000\n",
       "25            user.geo_enabled                 bool         2      0.000000\n",
       "26              user.protected                 bool         1      0.000000\n",
       "27                   user.lang               object         0      1.000000\n",
       "28              user.time_zone               object         0      1.000000\n",
       "29             user.created_at               object     30696      0.000000\n",
       "30                  created_at  datetime64[ns, UTC]    153004      0.000000\n",
       "31                timestamp_ms       datetime64[ns]    154810      0.000000\n",
       "32          place.country_code               object        63      0.981396\n",
       "33            place.place_type               object         4      0.981396\n",
       "34             place.full_name               object       741      0.981396\n",
       "35                   has_place                 bool         2      0.000000\n",
       "36           has_quoted_status                 bool         2      0.000000\n",
       "37            quoted_has_media                 bool         2      0.000000\n",
       "38         quoted_is_sensitive                 bool         2      0.000000\n",
       "39        quoted_user_verified                 bool         2      0.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== AUTO-DETECTED FEATURES ===\n",
      "Numeric (27): ['id_str', 'challenge_id', 'n_hashtags', 'n_mentions', 'n_urls', 'has_media', 'tweet_len_chars', 'tweet_len_words', 'retweet_count', 'favorite_count', 'reply_count', 'quote_count', 'is_reply', 'is_retweet', 'is_quote_status', 'user.favourites_count', 'user.statuses_count', 'user.listed_count', 'user.default_profile', 'user.default_profile_image'] ...\n",
      "Categorical (1): ['possibly_sensitive'] \n",
      "\n",
      "=== RUNNING UNIVARIATE ANALYSIS ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/eleves-a/2023/luis.zuin-ruiz/.local/lib/python3.9/site-packages/scipy/stats/_stats_py.py:5535: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  rpb, prob = pearsonr(x, y)\n",
      "/tmp/ipykernel_191494/515670510.py:198: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  sp_rho, sp_p = spearmanr(y_, x)\n",
      "/users/eleves-a/2023/luis.zuin-ruiz/.local/lib/python3.9/site-packages/scipy/stats/_stats_py.py:5535: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  rpb, prob = pearsonr(x, y)\n",
      "/tmp/ipykernel_191494/515670510.py:198: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  sp_rho, sp_p = spearmanr(y_, x)\n",
      "/users/eleves-a/2023/luis.zuin-ruiz/.local/lib/python3.9/site-packages/scipy/stats/_stats_py.py:5535: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  rpb, prob = pearsonr(x, y)\n",
      "/tmp/ipykernel_191494/515670510.py:198: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  sp_rho, sp_p = spearmanr(y_, x)\n",
      "/users/eleves-a/2023/luis.zuin-ruiz/.local/lib/python3.9/site-packages/scipy/stats/_stats_py.py:5535: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  rpb, prob = pearsonr(x, y)\n",
      "/tmp/ipykernel_191494/515670510.py:198: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  sp_rho, sp_p = spearmanr(y_, x)\n",
      "/users/eleves-a/2023/luis.zuin-ruiz/.local/lib/python3.9/site-packages/scipy/stats/_stats_py.py:5535: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  rpb, prob = pearsonr(x, y)\n",
      "/tmp/ipykernel_191494/515670510.py:198: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  sp_rho, sp_p = spearmanr(y_, x)\n",
      "/users/eleves-a/2023/luis.zuin-ruiz/.local/lib/python3.9/site-packages/scipy/stats/_stats_py.py:5535: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  rpb, prob = pearsonr(x, y)\n",
      "/tmp/ipykernel_191494/515670510.py:198: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  sp_rho, sp_p = spearmanr(y_, x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== NUMERIC FEATURES (sorted by significance) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>n</th>\n",
       "      <th>mean_0</th>\n",
       "      <th>mean_1</th>\n",
       "      <th>std_0</th>\n",
       "      <th>std_1</th>\n",
       "      <th>pointbiserial_r</th>\n",
       "      <th>pointbiserial_p</th>\n",
       "      <th>spearman_rho</th>\n",
       "      <th>spearman_p</th>\n",
       "      <th>mannwhitney_U</th>\n",
       "      <th>mannwhitney_p</th>\n",
       "      <th>logit_p</th>\n",
       "      <th>auc_univariate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>n_mentions</td>\n",
       "      <td>154914</td>\n",
       "      <td>8.612139e-01</td>\n",
       "      <td>3.971346e-01</td>\n",
       "      <td>1.298493e+00</td>\n",
       "      <td>8.895418e-01</td>\n",
       "      <td>-0.201323</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-0.227116</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.657434e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.387607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>is_reply</td>\n",
       "      <td>154914</td>\n",
       "      <td>4.105644e-01</td>\n",
       "      <td>2.010382e-01</td>\n",
       "      <td>4.919392e-01</td>\n",
       "      <td>4.007793e-01</td>\n",
       "      <td>-0.225437</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-0.225437</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.611869e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.395237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>user.favourites_count</td>\n",
       "      <td>154914</td>\n",
       "      <td>1.070028e+04</td>\n",
       "      <td>1.993850e+04</td>\n",
       "      <td>2.318324e+04</td>\n",
       "      <td>3.824825e+04</td>\n",
       "      <td>0.146453</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.115545</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.586861e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.566862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>user.statuses_count</td>\n",
       "      <td>154914</td>\n",
       "      <td>1.069337e+04</td>\n",
       "      <td>4.377145e+04</td>\n",
       "      <td>2.911889e+04</td>\n",
       "      <td>7.640890e+04</td>\n",
       "      <td>0.281050</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.455560</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.411770e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.763616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>user.default_profile</td>\n",
       "      <td>154914</td>\n",
       "      <td>7.443090e-01</td>\n",
       "      <td>4.253599e-01</td>\n",
       "      <td>4.362516e-01</td>\n",
       "      <td>4.944009e-01</td>\n",
       "      <td>-0.324203</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-0.324203</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.938626e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.340525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>user.geo_enabled</td>\n",
       "      <td>154914</td>\n",
       "      <td>2.499214e-01</td>\n",
       "      <td>5.396041e-01</td>\n",
       "      <td>4.329699e-01</td>\n",
       "      <td>4.984325e-01</td>\n",
       "      <td>0.296986</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.296986</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.121139e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.644841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>user.listed_count</td>\n",
       "      <td>154914</td>\n",
       "      <td>6.791010e+00</td>\n",
       "      <td>1.327866e+02</td>\n",
       "      <td>2.950026e+01</td>\n",
       "      <td>1.167234e+03</td>\n",
       "      <td>0.078584</td>\n",
       "      <td>1.075484e-210</td>\n",
       "      <td>0.615137</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>8.969074e+08</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.849824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>n_hashtags</td>\n",
       "      <td>154914</td>\n",
       "      <td>2.480224e-01</td>\n",
       "      <td>3.730482e-01</td>\n",
       "      <td>8.155625e-01</td>\n",
       "      <td>8.820414e-01</td>\n",
       "      <td>0.073421</td>\n",
       "      <td>4.137339e-184</td>\n",
       "      <td>0.111075</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.737765e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6.395986e-177</td>\n",
       "      <td>0.541595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>has_media</td>\n",
       "      <td>154914</td>\n",
       "      <td>2.255848e-02</td>\n",
       "      <td>4.595792e-02</td>\n",
       "      <td>1.484920e-01</td>\n",
       "      <td>2.093953e-01</td>\n",
       "      <td>0.064901</td>\n",
       "      <td>3.189894e-144</td>\n",
       "      <td>0.064901</td>\n",
       "      <td>3.189894e-144</td>\n",
       "      <td>2.916310e+09</td>\n",
       "      <td>6.340330e-144</td>\n",
       "      <td>2.891025e-138</td>\n",
       "      <td>0.511700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tweet_len_words</td>\n",
       "      <td>154914</td>\n",
       "      <td>2.429524e+01</td>\n",
       "      <td>2.334453e+01</td>\n",
       "      <td>1.483649e+01</td>\n",
       "      <td>1.387369e+01</td>\n",
       "      <td>-0.032928</td>\n",
       "      <td>1.963951e-38</td>\n",
       "      <td>-0.025501</td>\n",
       "      <td>1.032606e-23</td>\n",
       "      <td>3.074294e+09</td>\n",
       "      <td>1.049316e-23</td>\n",
       "      <td>2.189706e-38</td>\n",
       "      <td>0.485247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_str</td>\n",
       "      <td>154914</td>\n",
       "      <td>1.345951e+18</td>\n",
       "      <td>1.344952e+18</td>\n",
       "      <td>1.909453e+16</td>\n",
       "      <td>1.919989e+16</td>\n",
       "      <td>-0.026025</td>\n",
       "      <td>1.246522e-24</td>\n",
       "      <td>-0.027143</td>\n",
       "      <td>1.192728e-26</td>\n",
       "      <td>3.079992e+09</td>\n",
       "      <td>1.217623e-26</td>\n",
       "      <td>1.291627e-24</td>\n",
       "      <td>0.484293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>is_quote_status</td>\n",
       "      <td>154914</td>\n",
       "      <td>3.561216e-01</td>\n",
       "      <td>3.386351e-01</td>\n",
       "      <td>4.788547e-01</td>\n",
       "      <td>4.732488e-01</td>\n",
       "      <td>-0.018314</td>\n",
       "      <td>5.646311e-13</td>\n",
       "      <td>-0.018314</td>\n",
       "      <td>5.646311e-13</td>\n",
       "      <td>3.038403e+09</td>\n",
       "      <td>5.669978e-13</td>\n",
       "      <td>5.693127e-13</td>\n",
       "      <td>0.491257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>has_quoted_status</td>\n",
       "      <td>154914</td>\n",
       "      <td>3.559886e-01</td>\n",
       "      <td>3.385105e-01</td>\n",
       "      <td>4.788147e-01</td>\n",
       "      <td>4.732063e-01</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>5.766439e-13</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>5.766439e-13</td>\n",
       "      <td>3.038378e+09</td>\n",
       "      <td>5.790570e-13</td>\n",
       "      <td>5.814187e-13</td>\n",
       "      <td>0.491261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>quoted_user_verified</td>\n",
       "      <td>154914</td>\n",
       "      <td>1.262912e-01</td>\n",
       "      <td>1.169850e-01</td>\n",
       "      <td>3.321793e-01</td>\n",
       "      <td>3.214047e-01</td>\n",
       "      <td>-0.014187</td>\n",
       "      <td>2.347206e-08</td>\n",
       "      <td>-0.014187</td>\n",
       "      <td>2.347206e-08</td>\n",
       "      <td>3.013975e+09</td>\n",
       "      <td>2.350640e-08</td>\n",
       "      <td>2.366736e-08</td>\n",
       "      <td>0.495347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>has_place</td>\n",
       "      <td>154914</td>\n",
       "      <td>1.695817e-02</td>\n",
       "      <td>2.048726e-02</td>\n",
       "      <td>1.291154e-01</td>\n",
       "      <td>1.416609e-01</td>\n",
       "      <td>0.013029</td>\n",
       "      <td>2.922122e-07</td>\n",
       "      <td>0.013029</td>\n",
       "      <td>2.922122e-07</td>\n",
       "      <td>2.975646e+09</td>\n",
       "      <td>2.925120e-07</td>\n",
       "      <td>3.043416e-07</td>\n",
       "      <td>0.501765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>quoted_has_media</td>\n",
       "      <td>154914</td>\n",
       "      <td>4.019402e-02</td>\n",
       "      <td>4.518272e-02</td>\n",
       "      <td>1.964152e-01</td>\n",
       "      <td>2.077061e-01</td>\n",
       "      <td>0.012334</td>\n",
       "      <td>1.205450e-06</td>\n",
       "      <td>0.012334</td>\n",
       "      <td>1.205450e-06</td>\n",
       "      <td>2.971288e+09</td>\n",
       "      <td>1.206432e-06</td>\n",
       "      <td>1.222574e-06</td>\n",
       "      <td>0.502494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>n_urls</td>\n",
       "      <td>154914</td>\n",
       "      <td>6.148850e-01</td>\n",
       "      <td>6.270903e-01</td>\n",
       "      <td>5.092414e-01</td>\n",
       "      <td>5.115965e-01</td>\n",
       "      <td>0.011930</td>\n",
       "      <td>2.657094e-06</td>\n",
       "      <td>0.011172</td>\n",
       "      <td>1.096012e-05</td>\n",
       "      <td>2.953254e+09</td>\n",
       "      <td>1.096598e-05</td>\n",
       "      <td>2.661511e-06</td>\n",
       "      <td>0.505514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tweet_len_chars</td>\n",
       "      <td>154914</td>\n",
       "      <td>1.516591e+02</td>\n",
       "      <td>1.502686e+02</td>\n",
       "      <td>9.027169e+01</td>\n",
       "      <td>8.964460e+01</td>\n",
       "      <td>-0.007709</td>\n",
       "      <td>2.411260e-03</td>\n",
       "      <td>-0.007315</td>\n",
       "      <td>3.989669e-03</td>\n",
       "      <td>3.011464e+09</td>\n",
       "      <td>3.989981e-03</td>\n",
       "      <td>2.411938e-03</td>\n",
       "      <td>0.495767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>quoted_is_sensitive</td>\n",
       "      <td>154914</td>\n",
       "      <td>1.596633e-03</td>\n",
       "      <td>1.979513e-03</td>\n",
       "      <td>3.992621e-02</td>\n",
       "      <td>4.444797e-02</td>\n",
       "      <td>0.004537</td>\n",
       "      <td>7.411725e-02</td>\n",
       "      <td>0.004537</td>\n",
       "      <td>7.411725e-02</td>\n",
       "      <td>2.985042e+09</td>\n",
       "      <td>7.411747e-02</td>\n",
       "      <td>7.467318e-02</td>\n",
       "      <td>0.500191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>is_retweet</td>\n",
       "      <td>154914</td>\n",
       "      <td>2.540097e-04</td>\n",
       "      <td>4.014396e-04</td>\n",
       "      <td>1.593575e-02</td>\n",
       "      <td>2.003208e-02</td>\n",
       "      <td>0.004094</td>\n",
       "      <td>1.070610e-01</td>\n",
       "      <td>0.004094</td>\n",
       "      <td>1.070610e-01</td>\n",
       "      <td>2.985745e+09</td>\n",
       "      <td>1.070612e-01</td>\n",
       "      <td>1.101376e-01</td>\n",
       "      <td>0.500074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>challenge_id</td>\n",
       "      <td>154914</td>\n",
       "      <td>1.286277e+05</td>\n",
       "      <td>1.288108e+05</td>\n",
       "      <td>7.441562e+04</td>\n",
       "      <td>7.436494e+04</td>\n",
       "      <td>0.001228</td>\n",
       "      <td>6.289412e-01</td>\n",
       "      <td>0.001212</td>\n",
       "      <td>6.332905e-01</td>\n",
       "      <td>2.981996e+09</td>\n",
       "      <td>6.332890e-01</td>\n",
       "      <td>6.289387e-01</td>\n",
       "      <td>0.500701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>retweet_count</td>\n",
       "      <td>154914</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.986185e+09</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>favorite_count</td>\n",
       "      <td>154914</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.986185e+09</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>reply_count</td>\n",
       "      <td>154914</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.986185e+09</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>quote_count</td>\n",
       "      <td>154914</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.986185e+09</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>user.default_profile_image</td>\n",
       "      <td>154914</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.986185e+09</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>user.protected</td>\n",
       "      <td>154914</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.986185e+09</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       feature       n        mean_0        mean_1  \\\n",
       "3                   n_mentions  154914  8.612139e-01  3.971346e-01   \n",
       "12                    is_reply  154914  4.105644e-01  2.010382e-01   \n",
       "15       user.favourites_count  154914  1.070028e+04  1.993850e+04   \n",
       "16         user.statuses_count  154914  1.069337e+04  4.377145e+04   \n",
       "18        user.default_profile  154914  7.443090e-01  4.253599e-01   \n",
       "20            user.geo_enabled  154914  2.499214e-01  5.396041e-01   \n",
       "17           user.listed_count  154914  6.791010e+00  1.327866e+02   \n",
       "2                   n_hashtags  154914  2.480224e-01  3.730482e-01   \n",
       "5                    has_media  154914  2.255848e-02  4.595792e-02   \n",
       "7              tweet_len_words  154914  2.429524e+01  2.334453e+01   \n",
       "0                       id_str  154914  1.345951e+18  1.344952e+18   \n",
       "14             is_quote_status  154914  3.561216e-01  3.386351e-01   \n",
       "23           has_quoted_status  154914  3.559886e-01  3.385105e-01   \n",
       "26        quoted_user_verified  154914  1.262912e-01  1.169850e-01   \n",
       "22                   has_place  154914  1.695817e-02  2.048726e-02   \n",
       "24            quoted_has_media  154914  4.019402e-02  4.518272e-02   \n",
       "4                       n_urls  154914  6.148850e-01  6.270903e-01   \n",
       "6              tweet_len_chars  154914  1.516591e+02  1.502686e+02   \n",
       "25         quoted_is_sensitive  154914  1.596633e-03  1.979513e-03   \n",
       "13                  is_retweet  154914  2.540097e-04  4.014396e-04   \n",
       "1                 challenge_id  154914  1.286277e+05  1.288108e+05   \n",
       "8                retweet_count  154914  0.000000e+00  0.000000e+00   \n",
       "9               favorite_count  154914  0.000000e+00  0.000000e+00   \n",
       "10                 reply_count  154914  0.000000e+00  0.000000e+00   \n",
       "11                 quote_count  154914  0.000000e+00  0.000000e+00   \n",
       "19  user.default_profile_image  154914  0.000000e+00  0.000000e+00   \n",
       "21              user.protected  154914  0.000000e+00  0.000000e+00   \n",
       "\n",
       "           std_0         std_1  pointbiserial_r  pointbiserial_p  \\\n",
       "3   1.298493e+00  8.895418e-01        -0.201323     0.000000e+00   \n",
       "12  4.919392e-01  4.007793e-01        -0.225437     0.000000e+00   \n",
       "15  2.318324e+04  3.824825e+04         0.146453     0.000000e+00   \n",
       "16  2.911889e+04  7.640890e+04         0.281050     0.000000e+00   \n",
       "18  4.362516e-01  4.944009e-01        -0.324203     0.000000e+00   \n",
       "20  4.329699e-01  4.984325e-01         0.296986     0.000000e+00   \n",
       "17  2.950026e+01  1.167234e+03         0.078584    1.075484e-210   \n",
       "2   8.155625e-01  8.820414e-01         0.073421    4.137339e-184   \n",
       "5   1.484920e-01  2.093953e-01         0.064901    3.189894e-144   \n",
       "7   1.483649e+01  1.387369e+01        -0.032928     1.963951e-38   \n",
       "0   1.909453e+16  1.919989e+16        -0.026025     1.246522e-24   \n",
       "14  4.788547e-01  4.732488e-01        -0.018314     5.646311e-13   \n",
       "23  4.788147e-01  4.732063e-01        -0.018307     5.766439e-13   \n",
       "26  3.321793e-01  3.214047e-01        -0.014187     2.347206e-08   \n",
       "22  1.291154e-01  1.416609e-01         0.013029     2.922122e-07   \n",
       "24  1.964152e-01  2.077061e-01         0.012334     1.205450e-06   \n",
       "4   5.092414e-01  5.115965e-01         0.011930     2.657094e-06   \n",
       "6   9.027169e+01  8.964460e+01        -0.007709     2.411260e-03   \n",
       "25  3.992621e-02  4.444797e-02         0.004537     7.411725e-02   \n",
       "13  1.593575e-02  2.003208e-02         0.004094     1.070610e-01   \n",
       "1   7.441562e+04  7.436494e+04         0.001228     6.289412e-01   \n",
       "8   0.000000e+00  0.000000e+00              NaN              NaN   \n",
       "9   0.000000e+00  0.000000e+00              NaN              NaN   \n",
       "10  0.000000e+00  0.000000e+00              NaN              NaN   \n",
       "11  0.000000e+00  0.000000e+00              NaN              NaN   \n",
       "19  0.000000e+00  0.000000e+00              NaN              NaN   \n",
       "21  0.000000e+00  0.000000e+00              NaN              NaN   \n",
       "\n",
       "    spearman_rho     spearman_p  mannwhitney_U  mannwhitney_p        logit_p  \\\n",
       "3      -0.227116   0.000000e+00   3.657434e+09   0.000000e+00   0.000000e+00   \n",
       "12     -0.225437   0.000000e+00   3.611869e+09   0.000000e+00   0.000000e+00   \n",
       "15      0.115545   0.000000e+00   2.586861e+09   0.000000e+00   0.000000e+00   \n",
       "16      0.455560   0.000000e+00   1.411770e+09   0.000000e+00   0.000000e+00   \n",
       "18     -0.324203   0.000000e+00   3.938626e+09   0.000000e+00   0.000000e+00   \n",
       "20      0.296986   0.000000e+00   2.121139e+09   0.000000e+00   0.000000e+00   \n",
       "17      0.615137   0.000000e+00   8.969074e+08   0.000000e+00   0.000000e+00   \n",
       "2       0.111075   0.000000e+00   2.737765e+09   0.000000e+00  6.395986e-177   \n",
       "5       0.064901  3.189894e-144   2.916310e+09  6.340330e-144  2.891025e-138   \n",
       "7      -0.025501   1.032606e-23   3.074294e+09   1.049316e-23   2.189706e-38   \n",
       "0      -0.027143   1.192728e-26   3.079992e+09   1.217623e-26   1.291627e-24   \n",
       "14     -0.018314   5.646311e-13   3.038403e+09   5.669978e-13   5.693127e-13   \n",
       "23     -0.018307   5.766439e-13   3.038378e+09   5.790570e-13   5.814187e-13   \n",
       "26     -0.014187   2.347206e-08   3.013975e+09   2.350640e-08   2.366736e-08   \n",
       "22      0.013029   2.922122e-07   2.975646e+09   2.925120e-07   3.043416e-07   \n",
       "24      0.012334   1.205450e-06   2.971288e+09   1.206432e-06   1.222574e-06   \n",
       "4       0.011172   1.096012e-05   2.953254e+09   1.096598e-05   2.661511e-06   \n",
       "6      -0.007315   3.989669e-03   3.011464e+09   3.989981e-03   2.411938e-03   \n",
       "25      0.004537   7.411725e-02   2.985042e+09   7.411747e-02   7.467318e-02   \n",
       "13      0.004094   1.070610e-01   2.985745e+09   1.070612e-01   1.101376e-01   \n",
       "1       0.001212   6.332905e-01   2.981996e+09   6.332890e-01   6.289387e-01   \n",
       "8            NaN            NaN   2.986185e+09   1.000000e+00            NaN   \n",
       "9            NaN            NaN   2.986185e+09   1.000000e+00            NaN   \n",
       "10           NaN            NaN   2.986185e+09   1.000000e+00            NaN   \n",
       "11           NaN            NaN   2.986185e+09   1.000000e+00            NaN   \n",
       "19           NaN            NaN   2.986185e+09   1.000000e+00            NaN   \n",
       "21           NaN            NaN   2.986185e+09   1.000000e+00            NaN   \n",
       "\n",
       "    auc_univariate  \n",
       "3         0.387607  \n",
       "12        0.395237  \n",
       "15        0.566862  \n",
       "16        0.763616  \n",
       "18        0.340525  \n",
       "20        0.644841  \n",
       "17        0.849824  \n",
       "2         0.541595  \n",
       "5         0.511700  \n",
       "7         0.485247  \n",
       "0         0.484293  \n",
       "14        0.491257  \n",
       "23        0.491261  \n",
       "26        0.495347  \n",
       "22        0.501765  \n",
       "24        0.502494  \n",
       "4         0.505514  \n",
       "6         0.495767  \n",
       "25        0.500191  \n",
       "13        0.500074  \n",
       "1         0.500701  \n",
       "8              NaN  \n",
       "9              NaN  \n",
       "10             NaN  \n",
       "11             NaN  \n",
       "19             NaN  \n",
       "21             NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CATEGORICAL / BOOLEAN FEATURES (sorted by p-value) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>k_levels</th>\n",
       "      <th>n</th>\n",
       "      <th>test</th>\n",
       "      <th>chi2</th>\n",
       "      <th>df</th>\n",
       "      <th>p_value</th>\n",
       "      <th>cramers_v</th>\n",
       "      <th>top_levels_by_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>possibly_sensitive</td>\n",
       "      <td>2</td>\n",
       "      <td>154914</td>\n",
       "      <td>Fisher (2x2) + Chi2</td>\n",
       "      <td>53.178889</td>\n",
       "      <td>1</td>\n",
       "      <td>1.973608e-13</td>\n",
       "      <td>0.018528</td>\n",
       "      <td>{False: 0.467, 1.0: 0.348}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              feature  k_levels       n                 test       chi2  df  \\\n",
       "0  possibly_sensitive         2  154914  Fisher (2x2) + Chi2  53.178889   1   \n",
       "\n",
       "        p_value  cramers_v          top_levels_by_rate  \n",
       "0  1.973608e-13   0.018528  {False: 0.467, 1.0: 0.348}  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1) Quick full analysis using auto-detected features\n",
    "num_res, cat_res = full_influencer_feature_analysis(train_clean, label_col=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6129cec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-level split:\n",
      "  #train tweets: 139426\n",
      "  #val tweets:   15488\n",
      "  #unique users train: 44158\n",
      "  #unique users val:   4907\n",
      "\n",
      "Train label distribution:\n",
      "label\n",
      "0    0.534556\n",
      "1    0.465444\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Val label distribution:\n",
      "label\n",
      "0    0.525762\n",
      "1    0.474238\n",
      "Name: proportion, dtype: float64\n",
      "[all_9_meta] acc=0.8032, AUC=0.8740, n_feats=9\n",
      "[all_plus_source_idx] acc=0.8085, AUC=0.8787, n_feats=10\n",
      "[all_plus_source_app] acc=0.8077, AUC=0.8789, n_feats=10\n",
      "[all_9_meta_non_log] acc=0.7788, AUC=0.8542, n_feats=9\n",
      "[all_with_len] acc=0.8017, AUC=0.8741, n_feats=10\n",
      "[all_minus_fav] acc=0.7994, AUC=0.8690, n_feats=8\n",
      "[all_minus_hashs] acc=0.8029, AUC=0.8741, n_feats=8\n",
      "[all_minus_hashs_media] acc=0.8028, AUC=0.8738, n_feats=7\n",
      "[all_minus_hashs_media_with_src] acc=0.8096, AUC=0.8786, n_feats=8\n",
      "[best_with_nine] acc=0.7898, AUC=0.8624, n_feats=9\n",
      "[best_with_eight] acc=0.7890, AUC=0.8621, n_feats=8\n",
      "[best_with_seven] acc=0.8119, AUC=0.8785, n_feats=7\n",
      "[best_with_six] acc=0.8111, AUC=0.8784, n_feats=6\n",
      "[second_best_with_six] acc=0.8110, AUC=0.8781, n_feats=6\n",
      "[best_with_five] acc=0.8089, AUC=0.8756, n_feats=5\n",
      "[current] acc=0.7949, AUC=0.8673, n_feats=3\n",
      "[current_with_src] acc=0.8080, AUC=0.8749, n_feats=4\n",
      "[cur_plus1] acc=0.7959, AUC=0.8675, n_feats=4\n",
      "[user_stats_3] acc=0.7931, AUC=0.8702, n_feats=3\n",
      "[user_stats_2] acc=0.7936, AUC=0.8647, n_feats=2\n",
      "[log_status_only] acc=0.7053, AUC=0.7774, n_feats=1\n",
      "[log_listed_only] acc=0.7872, AUC=0.8481, n_feats=1\n",
      "[interaction_only] acc=0.5910, AUC=0.6288, n_feats=4\n",
      "[reply_mentions_only] acc=0.5905, AUC=0.6128, n_feats=2\n",
      "[hashtags_media_only] acc=0.5661, AUC=0.5504, n_feats=2\n",
      "[prev_best_minus_log_status] acc=0.7979, AUC=0.8667, n_feats=9\n",
      "[prev_best_minus_log_listed] acc=0.7565, AUC=0.8336, n_feats=9\n",
      "[prev_best_minus_log_fav] acc=0.8069, AUC=0.8760, n_feats=9\n",
      "[prev_best_minus_user.default_profile] acc=0.8094, AUC=0.8786, n_feats=9\n",
      "[prev_best_minus_user.geo_enabled] acc=0.8111, AUC=0.8787, n_feats=9\n",
      "[prev_best_minus_is_reply] acc=0.8089, AUC=0.8785, n_feats=9\n",
      "[prev_best_minus_n_mentions] acc=0.8080, AUC=0.8787, n_feats=9\n",
      "[prev_best_minus_n_hashtags] acc=0.8088, AUC=0.8788, n_feats=9\n",
      "[prev_best_minus_has_media] acc=0.8095, AUC=0.8786, n_feats=9\n",
      "[prev_best_minus_source_idx] acc=0.8032, AUC=0.8740, n_feats=9\n",
      "[single_user.statuses_count] acc=0.6972, AUC=0.7774, n_feats=1\n",
      "[single_user.listed_count] acc=0.7436, AUC=0.8481, n_feats=1\n",
      "[single_user.favourites_count] acc=0.5870, AUC=0.5815, n_feats=1\n",
      "[single_user.default_profile] acc=0.6665, AUC=0.6622, n_feats=1\n",
      "[single_user.geo_enabled] acc=0.6454, AUC=0.6396, n_feats=1\n",
      "[single_is_reply] acc=0.5925, AUC=0.6030, n_feats=1\n",
      "[single_n_mentions] acc=0.5967, AUC=0.6108, n_feats=1\n",
      "[single_n_hashtags] acc=0.5593, AUC=0.5419, n_feats=1\n",
      "[single_has_media] acc=0.5354, AUC=0.5113, n_feats=1\n",
      "\n",
      "=== Summary (sorted by accuracy, then AUC) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subset</th>\n",
       "      <th>n_features</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>best_with_seven</td>\n",
       "      <td>7</td>\n",
       "      <td>0.811919</td>\n",
       "      <td>0.878462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>prev_best_minus_user.geo_enabled</td>\n",
       "      <td>9</td>\n",
       "      <td>0.811080</td>\n",
       "      <td>0.878673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>best_with_six</td>\n",
       "      <td>6</td>\n",
       "      <td>0.811080</td>\n",
       "      <td>0.878410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>second_best_with_six</td>\n",
       "      <td>6</td>\n",
       "      <td>0.810950</td>\n",
       "      <td>0.878136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>all_minus_hashs_media_with_src</td>\n",
       "      <td>8</td>\n",
       "      <td>0.809595</td>\n",
       "      <td>0.878615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>prev_best_minus_has_media</td>\n",
       "      <td>9</td>\n",
       "      <td>0.809465</td>\n",
       "      <td>0.878563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>prev_best_minus_user.default_profile</td>\n",
       "      <td>9</td>\n",
       "      <td>0.809401</td>\n",
       "      <td>0.878559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>prev_best_minus_is_reply</td>\n",
       "      <td>9</td>\n",
       "      <td>0.808949</td>\n",
       "      <td>0.878471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>best_with_five</td>\n",
       "      <td>5</td>\n",
       "      <td>0.808884</td>\n",
       "      <td>0.875568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>prev_best_minus_n_hashtags</td>\n",
       "      <td>9</td>\n",
       "      <td>0.808820</td>\n",
       "      <td>0.878783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>all_plus_source_idx</td>\n",
       "      <td>10</td>\n",
       "      <td>0.808497</td>\n",
       "      <td>0.878743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>prev_best_minus_n_mentions</td>\n",
       "      <td>9</td>\n",
       "      <td>0.808045</td>\n",
       "      <td>0.878710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>current_with_src</td>\n",
       "      <td>4</td>\n",
       "      <td>0.808045</td>\n",
       "      <td>0.874881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>all_plus_source_app</td>\n",
       "      <td>10</td>\n",
       "      <td>0.807658</td>\n",
       "      <td>0.878864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>prev_best_minus_log_fav</td>\n",
       "      <td>9</td>\n",
       "      <td>0.806883</td>\n",
       "      <td>0.875990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>all_9_meta</td>\n",
       "      <td>9</td>\n",
       "      <td>0.803202</td>\n",
       "      <td>0.873994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>prev_best_minus_source_idx</td>\n",
       "      <td>9</td>\n",
       "      <td>0.803202</td>\n",
       "      <td>0.873994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>all_minus_hashs</td>\n",
       "      <td>8</td>\n",
       "      <td>0.802944</td>\n",
       "      <td>0.874141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>all_minus_hashs_media</td>\n",
       "      <td>7</td>\n",
       "      <td>0.802751</td>\n",
       "      <td>0.873816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>all_with_len</td>\n",
       "      <td>10</td>\n",
       "      <td>0.801653</td>\n",
       "      <td>0.874050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>all_minus_fav</td>\n",
       "      <td>8</td>\n",
       "      <td>0.799393</td>\n",
       "      <td>0.868952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>prev_best_minus_log_status</td>\n",
       "      <td>9</td>\n",
       "      <td>0.797908</td>\n",
       "      <td>0.866729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>cur_plus1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.795907</td>\n",
       "      <td>0.867451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>current</td>\n",
       "      <td>3</td>\n",
       "      <td>0.794938</td>\n",
       "      <td>0.867276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>user_stats_2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.793647</td>\n",
       "      <td>0.864671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>user_stats_3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.793130</td>\n",
       "      <td>0.870185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>best_with_nine</td>\n",
       "      <td>9</td>\n",
       "      <td>0.789773</td>\n",
       "      <td>0.862425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>best_with_eight</td>\n",
       "      <td>8</td>\n",
       "      <td>0.788998</td>\n",
       "      <td>0.862108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>log_listed_only</td>\n",
       "      <td>1</td>\n",
       "      <td>0.787190</td>\n",
       "      <td>0.848116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>all_9_meta_non_log</td>\n",
       "      <td>9</td>\n",
       "      <td>0.778796</td>\n",
       "      <td>0.854170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>prev_best_minus_log_listed</td>\n",
       "      <td>9</td>\n",
       "      <td>0.756457</td>\n",
       "      <td>0.833565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>single_user.listed_count</td>\n",
       "      <td>1</td>\n",
       "      <td>0.743608</td>\n",
       "      <td>0.848116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>log_status_only</td>\n",
       "      <td>1</td>\n",
       "      <td>0.705256</td>\n",
       "      <td>0.777398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>single_user.statuses_count</td>\n",
       "      <td>1</td>\n",
       "      <td>0.697185</td>\n",
       "      <td>0.777397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>single_user.default_profile</td>\n",
       "      <td>1</td>\n",
       "      <td>0.666516</td>\n",
       "      <td>0.662209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>single_user.geo_enabled</td>\n",
       "      <td>1</td>\n",
       "      <td>0.645403</td>\n",
       "      <td>0.639589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>single_n_mentions</td>\n",
       "      <td>1</td>\n",
       "      <td>0.596655</td>\n",
       "      <td>0.610834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>single_is_reply</td>\n",
       "      <td>1</td>\n",
       "      <td>0.592523</td>\n",
       "      <td>0.602956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>interaction_only</td>\n",
       "      <td>4</td>\n",
       "      <td>0.591038</td>\n",
       "      <td>0.628846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>reply_mentions_only</td>\n",
       "      <td>2</td>\n",
       "      <td>0.590522</td>\n",
       "      <td>0.612826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>single_user.favourites_count</td>\n",
       "      <td>1</td>\n",
       "      <td>0.586971</td>\n",
       "      <td>0.581451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>hashtags_media_only</td>\n",
       "      <td>2</td>\n",
       "      <td>0.566116</td>\n",
       "      <td>0.550390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>single_n_hashtags</td>\n",
       "      <td>1</td>\n",
       "      <td>0.559272</td>\n",
       "      <td>0.541919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>single_has_media</td>\n",
       "      <td>1</td>\n",
       "      <td>0.535382</td>\n",
       "      <td>0.511284</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  subset  n_features  accuracy       auc\n",
       "0                        best_with_seven           7  0.811919  0.878462\n",
       "1       prev_best_minus_user.geo_enabled           9  0.811080  0.878673\n",
       "2                          best_with_six           6  0.811080  0.878410\n",
       "3                   second_best_with_six           6  0.810950  0.878136\n",
       "4         all_minus_hashs_media_with_src           8  0.809595  0.878615\n",
       "5              prev_best_minus_has_media           9  0.809465  0.878563\n",
       "6   prev_best_minus_user.default_profile           9  0.809401  0.878559\n",
       "7               prev_best_minus_is_reply           9  0.808949  0.878471\n",
       "8                         best_with_five           5  0.808884  0.875568\n",
       "9             prev_best_minus_n_hashtags           9  0.808820  0.878783\n",
       "10                   all_plus_source_idx          10  0.808497  0.878743\n",
       "11            prev_best_minus_n_mentions           9  0.808045  0.878710\n",
       "12                      current_with_src           4  0.808045  0.874881\n",
       "13                   all_plus_source_app          10  0.807658  0.878864\n",
       "14               prev_best_minus_log_fav           9  0.806883  0.875990\n",
       "15                            all_9_meta           9  0.803202  0.873994\n",
       "16            prev_best_minus_source_idx           9  0.803202  0.873994\n",
       "17                       all_minus_hashs           8  0.802944  0.874141\n",
       "18                 all_minus_hashs_media           7  0.802751  0.873816\n",
       "19                          all_with_len          10  0.801653  0.874050\n",
       "20                         all_minus_fav           8  0.799393  0.868952\n",
       "21            prev_best_minus_log_status           9  0.797908  0.866729\n",
       "22                             cur_plus1           4  0.795907  0.867451\n",
       "23                               current           3  0.794938  0.867276\n",
       "24                          user_stats_2           2  0.793647  0.864671\n",
       "25                          user_stats_3           3  0.793130  0.870185\n",
       "26                        best_with_nine           9  0.789773  0.862425\n",
       "27                       best_with_eight           8  0.788998  0.862108\n",
       "28                       log_listed_only           1  0.787190  0.848116\n",
       "29                    all_9_meta_non_log           9  0.778796  0.854170\n",
       "30            prev_best_minus_log_listed           9  0.756457  0.833565\n",
       "31              single_user.listed_count           1  0.743608  0.848116\n",
       "32                       log_status_only           1  0.705256  0.777398\n",
       "33            single_user.statuses_count           1  0.697185  0.777397\n",
       "34           single_user.default_profile           1  0.666516  0.662209\n",
       "35               single_user.geo_enabled           1  0.645403  0.639589\n",
       "36                     single_n_mentions           1  0.596655  0.610834\n",
       "37                       single_is_reply           1  0.592523  0.602956\n",
       "38                      interaction_only           4  0.591038  0.628846\n",
       "39                   reply_mentions_only           2  0.590522  0.612826\n",
       "40          single_user.favourites_count           1  0.586971  0.581451\n",
       "41                   hashtags_media_only           2  0.566116  0.550390\n",
       "42                     single_n_hashtags           1  0.559272  0.541919\n",
       "43                      single_has_media           1  0.535382  0.511284"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, ast, json, hashlib\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1. Parsing + author_pseudo_id + basic metadata columns\n",
    "# ============================================================\n",
    "\n",
    "def parse_tweets_meta(path, expect_label=True):\n",
    "    \"\"\"\n",
    "    Parse the raw train.jsonl and build a flat DataFrame with:\n",
    "    - author_pseudo_id\n",
    "    - basic tweet/user fields\n",
    "    - simple structural flags (is_reply, has_media, etc.)\n",
    "    \"\"\"\n",
    "    path = Path(path)\n",
    "    df = pd.read_json(path, lines=True)\n",
    "    df = pd.json_normalize(df.to_dict(orient=\"records\"), sep=\".\")\n",
    "\n",
    "    # Ensure some nested columns exist\n",
    "    for col in [\n",
    "        \"text\", \"extended_tweet.full_text\", \"source\",\n",
    "        \"entities.hashtags\", \"entities.user_mentions\", \"entities.urls\",\n",
    "        \"extended_entities.media\",\n",
    "        \"user.created_at\", \"user.description\", \"user.url\", \"user.location\",\n",
    "        \"user.favourites_count\", \"user.statuses_count\", \"user.listed_count\",\n",
    "        \"user.default_profile\", \"user.geo_enabled\",\n",
    "        \"in_reply_to_status_id\", \"in_reply_to_user_id\",\n",
    "    ]:\n",
    "        if col not in df.columns:\n",
    "            df[col] = np.nan\n",
    "\n",
    "    # -------- full text ----------\n",
    "    df[\"full_text\"] = df[\"extended_tweet.full_text\"].fillna(df[\"text\"]).fillna(\"\")\n",
    "\n",
    "    df[\"text_len\"] = df[\"full_text\"].str.len()\n",
    "\n",
    "    # -------- counts / list lengths ----------\n",
    "    def safe_len(x):\n",
    "        if isinstance(x, list):\n",
    "            return len(x)\n",
    "        if isinstance(x, str):\n",
    "            try:\n",
    "                v = ast.literal_eval(x)\n",
    "                return len(v) if isinstance(v, (list, tuple)) else 1\n",
    "            except Exception:\n",
    "                return 0\n",
    "        return 0\n",
    "\n",
    "    df[\"n_hashtags\"] = df[\"entities.hashtags\"].apply(safe_len)\n",
    "    df[\"n_mentions\"] = df[\"entities.user_mentions\"].apply(safe_len)\n",
    "    df[\"n_urls\"]     = df[\"entities.urls\"].apply(safe_len)\n",
    "\n",
    "    # media flag\n",
    "    df[\"has_media\"] = df[\"extended_entities.media\"].apply(lambda x: safe_len(x) > 0)\n",
    "\n",
    "    # -------- author_pseudo_id (same logic you used) ----------\n",
    "    def make_user_key(row):\n",
    "        key = (\n",
    "            str(row.get(\"user.created_at\", \"\")) + \"|\" +\n",
    "            str(row.get(\"user.description\", \"\")) + \"|\" +\n",
    "            str(row.get(\"user.url\", \"\")) + \"|\" +\n",
    "            str(row.get(\"user.location\", \"\"))\n",
    "        )\n",
    "        return hashlib.md5(key.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "    df[\"author_pseudo_id\"] = df.apply(make_user_key, axis=1)\n",
    "\n",
    "    # -------- structural flags ----------\n",
    "    df[\"is_reply\"] = (\n",
    "        df[\"in_reply_to_status_id\"].notna() |\n",
    "        df[\"in_reply_to_user_id\"].notna()\n",
    "    )\n",
    "\n",
    "    # note: in your real raw json you might have a better retweet flag;\n",
    "    # here we approximate using RT prefix\n",
    "    df[\"is_retweet\"] = df[\"text\"].fillna(\"\").str.startswith(\"RT @\")\n",
    "\n",
    "    # ensure boolean columns exist (even if missing)\n",
    "    for col in [\"user.default_profile\", \"user.geo_enabled\"]:\n",
    "        if col not in df.columns:\n",
    "            df[col] = np.nan\n",
    "\n",
    "    # -------- source_app (HTML â readable name) ----------\n",
    "    def extract_source(x):\n",
    "        if not isinstance(x, str):\n",
    "            return \"Unknown\"\n",
    "        m = re.search(r'>([^<]+)<', x)\n",
    "        return m.group(1) if m else x\n",
    "\n",
    "    df[\"source_app\"] = df[\"source\"].apply(extract_source)\n",
    "\n",
    "    # keep only metadata we care about:\n",
    "    keep_cols = [\n",
    "        \"author_pseudo_id\", \"full_text\",\n",
    "        \"challenge_id\" if \"challenge_id\" in df.columns else None,\n",
    "        \"n_hashtags\", \"n_mentions\", \"n_urls\", \"has_media\",\n",
    "        \"user.favourites_count\", \"user.statuses_count\", \"user.listed_count\",\n",
    "        \"user.default_profile\", \"user.geo_enabled\",\n",
    "        \"is_reply\", \"text_len\", \"source_app\"\n",
    "    ]\n",
    "    keep_cols = [c for c in keep_cols if c is not None]\n",
    "\n",
    "    out = df[keep_cols].copy()\n",
    "\n",
    "    # attach label if present\n",
    "    if expect_label and \"label\" in df.columns:\n",
    "        out[\"label\"] = df[\"label\"]\n",
    "    elif expect_label and \"label\" not in df.columns:\n",
    "        print(\"Warning: 'label' not found in this file; returning features only.\")\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. Feature engineering for metadata-only baselines\n",
    "# ============================================================\n",
    "\n",
    "def build_meta_features(df, fit_stats=None, src2idx=None, K=15):\n",
    "    \"\"\"\n",
    "    Build metadata features:\n",
    "      - log_status (from user.statuses_count)\n",
    "      - log_listed (from user.listed_count)\n",
    "      - log_fav   (from user.favourites_count)\n",
    "      - n_mentions, n_hashtags (not capped here, but can be)\n",
    "      - booleans as 0/1\n",
    "      - source_idx: bucketized from source_app with top-K on TRAIN\n",
    "    fit_stats: dict with p99s, learned from TRAIN\n",
    "    src2idx: dict source_app -> index, learned from TRAIN\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # ensure numeric columns exist\n",
    "    for col in [\"user.statuses_count\", \"user.listed_count\", \"user.favourites_count\"]:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0\n",
    "\n",
    "    # fit_stats = dict with p99s, learned from train set only\n",
    "    if fit_stats is None:\n",
    "        fit_stats = {}\n",
    "        for col in [\"user.statuses_count\", \"user.listed_count\", \"user.favourites_count\"]:\n",
    "            fit_stats[f\"{col}_p99\"] = float(df[col].quantile(0.995))\n",
    "\n",
    "    # log transforms with clipping\n",
    "    df[\"log_status\"] = np.log1p(\n",
    "        np.clip(df[\"user.statuses_count\"].fillna(0), 0, fit_stats[\"user.statuses_count_p99\"])\n",
    "    )\n",
    "    df[\"log_listed\"] = np.log1p(\n",
    "        np.clip(df[\"user.listed_count\"].fillna(0), 0, fit_stats[\"user.listed_count_p99\"])\n",
    "    )\n",
    "    df[\"log_fav\"] = np.log1p(\n",
    "        np.clip(df[\"user.favourites_count\"].fillna(0), 0, fit_stats[\"user.favourites_count_p99\"])\n",
    "    )\n",
    "\n",
    "    # counts: small cap to reduce outliers\n",
    "    df[\"n_mentions\"] = df[\"n_mentions\"].fillna(0).astype(int)#.clip(0, 10)\n",
    "    df[\"n_hashtags\"] = df[\"n_hashtags\"].fillna(0).astype(int)#.clip(0, 10)\n",
    "\n",
    "    # booleans â 0/1\n",
    "    for bcol in [\"has_media\", \"is_reply\", \"user.default_profile\", \"user.geo_enabled\"]:\n",
    "        if bcol not in df.columns:\n",
    "            df[bcol] = False\n",
    "        df[bcol] = df[bcol].fillna(False).astype(int)\n",
    "\n",
    "    # ---- source_idx from source_app ----\n",
    "    if \"source_app\" not in df.columns:\n",
    "        df[\"source_app\"] = \"Unknown\"\n",
    "\n",
    "    if src2idx is None:\n",
    "        top_src = df[\"source_app\"].fillna(\"Unknown\").value_counts().head(K).index.tolist()\n",
    "        src2idx = {s: i+1 for i, s in enumerate(top_src)}  # 0 reserved for \"Other\"\n",
    "\n",
    "    df[\"source_idx\"] = (\n",
    "        df[\"source_app\"]\n",
    "        .fillna(\"Unknown\")\n",
    "        .map(src2idx)\n",
    "        .fillna(0)\n",
    "        .astype(int)\n",
    "    )\n",
    "\n",
    "    return df, fit_stats, src2idx\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. Author-based split (same spirit as your TweetsDataModule)\n",
    "# ============================================================\n",
    "\n",
    "def author_based_split(df, val_size=0.1, random_state=42):\n",
    "    \"\"\"\n",
    "    Split df into train/val such that authors (author_pseudo_id)\n",
    "    do not overlap between train and val.\n",
    "    \"\"\"\n",
    "    assert \"author_pseudo_id\" in df.columns, \"author_pseudo_id column missing\"\n",
    "\n",
    "    user_ids = df[\"author_pseudo_id\"].astype(str)\n",
    "    unique_users = user_ids.unique()\n",
    "\n",
    "    train_users, val_users = train_test_split(\n",
    "        unique_users, test_size=val_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    train_mask = user_ids.isin(train_users)\n",
    "    val_mask   = user_ids.isin(val_users)\n",
    "\n",
    "    train_df = df[train_mask].reset_index(drop=True)\n",
    "    val_df   = df[val_mask].reset_index(drop=True)\n",
    "\n",
    "    print(\"User-level split:\")\n",
    "    print(\"  #train tweets:\", len(train_df))\n",
    "    print(\"  #val tweets:  \", len(val_df))\n",
    "    print(\"  #unique users train:\", len(np.unique(user_ids[train_mask])))\n",
    "    print(\"  #unique users val:  \", len(np.unique(user_ids[val_mask])))\n",
    "\n",
    "    return train_df, val_df\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4. Run LogisticRegression baselines for multiple subsets\n",
    "# ============================================================\n",
    "\n",
    "def run_metadata_ablations(train_df, val_df):\n",
    "    \"\"\"\n",
    "    Train a LogisticRegression on various subsets of metadata\n",
    "    and evaluate on author-based validation set.\n",
    "    \"\"\"\n",
    "\n",
    "    # All meta features we care about\n",
    "    ALL_META_FEATURES_LOG = [\n",
    "        \"log_status\",\n",
    "        \"log_listed\",\n",
    "        \"log_fav\",\n",
    "        \"user.default_profile\",\n",
    "        \"user.geo_enabled\",\n",
    "        \"is_reply\",\n",
    "        \"n_mentions\",\n",
    "        \"n_hashtags\",\n",
    "        \"has_media\",\n",
    "\n",
    "    ]\n",
    "\n",
    "    ALL_META_FEATURES = [\n",
    "        \"user.statuses_count\",\n",
    "        \"user.listed_count\",\n",
    "        \"user.favourites_count\",\n",
    "        \"user.default_profile\",\n",
    "        \"user.geo_enabled\",\n",
    "        \"is_reply\",\n",
    "        \"n_mentions\",\n",
    "        \"n_hashtags\",\n",
    "        \"has_media\",\n",
    "\n",
    "    ]\n",
    "\n",
    "    # Define ablation sets: you can add/remove sets here easily\n",
    "    ablation_sets = {\n",
    "        \"all_9_meta\": ALL_META_FEATURES_LOG,\n",
    "        \"all_plus_source_idx\": ALL_META_FEATURES_LOG + [\"source_idx\"],\n",
    "        \"all_plus_source_app\": ALL_META_FEATURES_LOG + [\"source_app\"],\n",
    "        \"all_9_meta_non_log\": ALL_META_FEATURES,\n",
    "        \"all_with_len\": ALL_META_FEATURES_LOG + [\"text_len\"],\n",
    "        \"all_minus_fav\": [\n",
    "            \"log_status\",\n",
    "            \"log_listed\",\n",
    "            \"user.default_profile\",\n",
    "            \"user.geo_enabled\",\n",
    "            \"is_reply\",\n",
    "            \"n_mentions\",\n",
    "            \"n_hashtags\",\n",
    "            \"has_media\"],\n",
    "        \"all_minus_hashs\": [\n",
    "            \"log_status\",\n",
    "            \"log_listed\",\n",
    "            \"user.default_profile\",\n",
    "            \"user.geo_enabled\",\n",
    "            \"is_reply\",\n",
    "            \"n_mentions\",\n",
    "            \"log_fav\",\n",
    "            \"has_media\"],\n",
    "        \"all_minus_hashs_media\": [\n",
    "            \"log_status\",\n",
    "            \"log_listed\",\n",
    "            \"user.default_profile\",\n",
    "            \"user.geo_enabled\",\n",
    "            \"is_reply\",\n",
    "            \"n_mentions\",\n",
    "            \"log_fav\"],\n",
    "        \"all_minus_hashs_media_with_src\": [\n",
    "            \"log_status\",\n",
    "            \"log_listed\",\n",
    "            \"user.default_profile\",\n",
    "            \"user.geo_enabled\",\n",
    "            \"is_reply\",\n",
    "            \"n_mentions\",\n",
    "            \"log_fav\", \"source_idx\"],\n",
    "\n",
    "        \"best_with_nine\":[\n",
    "            \"user.statuses_count\",\n",
    "            \"user.listed_count\",\n",
    "            \"user.favourites_count\",\n",
    "            \"user.default_profile\",\n",
    "            \"is_reply\",\n",
    "            \"n_mentions\",\n",
    "            \"n_hashtags\",\n",
    "            \"has_media\",\n",
    "            \"source_idx\"\n",
    "        ],\n",
    "        \"best_with_eight\":[\n",
    "            \"user.statuses_count\",\n",
    "            \"user.listed_count\",\n",
    "            \"user.favourites_count\",\n",
    "            \"user.default_profile\",\n",
    "            \"is_reply\",\n",
    "            \"n_mentions\",\n",
    "            \"n_hashtags\",\n",
    "            \"source_idx\"\n",
    "        ],\n",
    "        \"best_with_seven\": [\n",
    "            \"log_status\",\n",
    "            \"log_listed\",\n",
    "            \"user.default_profile\",\n",
    "            \"is_reply\",\n",
    "            \"n_mentions\",\n",
    "            \"log_fav\", \"source_idx\"],\n",
    "        \n",
    "        \"best_with_six\": [\n",
    "            \"log_status\",\n",
    "            \"log_listed\",\n",
    "            \"user.default_profile\",\n",
    "            \"is_reply\",\n",
    "            #\"n_mentions\",\n",
    "            \"log_fav\", \"source_idx\"],\n",
    "\n",
    "        \"second_best_with_six\": [\n",
    "            \"log_status\",\n",
    "            \"log_listed\",\n",
    "            \"user.default_profile\",\n",
    "            #\"is_reply\",\n",
    "            \"n_mentions\",\n",
    "            \"log_fav\", \"source_idx\"],\n",
    "        \"best_with_five\": [\n",
    "            \"log_status\",\n",
    "            \"log_listed\",\n",
    "            \"user.default_profile\",\n",
    "            \"is_reply\",\n",
    "            #\"n_mentions\",\n",
    "            #\"log_fav\", \n",
    "            \"source_idx\"],\n",
    "        \"current\": [\"log_status\", \"log_listed\", \"n_mentions\"],\n",
    "        \"current_with_src\": [\"log_status\", \"log_listed\", \"n_mentions\", \"source_idx\"],\n",
    "        \"cur_plus1\": [\"log_status\", \"log_listed\", \"n_mentions\", \"user.default_profile\"],\n",
    "        \"user_stats_3\": [\"log_status\", \"log_listed\", \"log_fav\"],\n",
    "        \"user_stats_2\": [\"log_status\", \"log_listed\"],\n",
    "        \"log_status_only\": [\"log_status\"],\n",
    "        \"log_listed_only\": [\"log_listed\"],\n",
    "        \"interaction_only\": [\"is_reply\", \"n_mentions\", \"n_hashtags\", \"has_media\"],\n",
    "        \"reply_mentions_only\": [\"is_reply\", \"n_mentions\"],\n",
    "        \"hashtags_media_only\": [\"n_hashtags\", \"has_media\"],\n",
    "    }\n",
    "\n",
    "    prev_best_res = ALL_META_FEATURES_LOG.copy()\n",
    "    prev_best_res += [\"source_idx\"]\n",
    "    #prev_best_res.remove(\"user.geo_enabled\")\n",
    "    #prev_best_res.remove(\"has_media\")\n",
    "    \n",
    "    # to_rmv = [\"log_status\",\n",
    "    #         \"log_listed\",\n",
    "    #         \"user.default_profile\",\n",
    "    #         \"user.geo_enabled\",\n",
    "    #         \"is_reply\",\n",
    "    #         \"n_mentions\",\n",
    "    #         \"log_fav\"]\n",
    "    to_rmv = prev_best_res.copy()\n",
    "    for col in to_rmv:\n",
    "        cur = prev_best_res.copy()\n",
    "        if col in cur:\n",
    "            cur.remove(col)\n",
    "        else:\n",
    "            continue\n",
    "        ablation_sets[f\"prev_best_minus_{col}\"] = cur\n",
    "    # also: each single feature as its own baseline\n",
    "    for feat in ALL_META_FEATURES:\n",
    "        ablation_sets.setdefault(f\"single_{feat}\", [feat])\n",
    "\n",
    "    y_train = train_df[\"label\"].astype(int).values\n",
    "    y_val   = val_df[\"label\"].astype(int).values\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for name, cols in ablation_sets.items():\n",
    "        # some columns might be missing if anything went wrong, so check:\n",
    "        missing = [c for c in cols if c not in train_df.columns]\n",
    "        if missing:\n",
    "            print(f\"[{name}] Skipping: missing columns {missing}\")\n",
    "            continue\n",
    "\n",
    "        # split into numeric vs categorical (only source_idx is categorical here)\n",
    "        cat_cols = [c for c in cols if c in [\"source_idx\", \"source_app\"]]\n",
    "        num_cols = [c for c in cols if c not in [\"source_idx\", \"source_app\"]]\n",
    "\n",
    "        # prepare ColumnTransformer\n",
    "        transformers = []\n",
    "        if num_cols:\n",
    "            transformers.append((\"num\", StandardScaler(), num_cols))\n",
    "        if cat_cols:\n",
    "            transformers.append((\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols))\n",
    "\n",
    "        if not transformers:\n",
    "            print(f\"[{name}] Skipping: no valid columns after split.\")\n",
    "            continue\n",
    "\n",
    "        pre = ColumnTransformer(transformers=transformers, remainder=\"drop\")\n",
    "\n",
    "        clf = Pipeline(\n",
    "            steps=[\n",
    "                (\"pre\", pre),\n",
    "                (\"logreg\", LogisticRegression(max_iter=2000, class_weight=\"balanced\")),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        X_train = train_df[cols]\n",
    "        X_val   = val_df[cols]\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_val)\n",
    "        y_prob = clf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "        acc = accuracy_score(y_val, y_pred)\n",
    "        auc = roc_auc_score(y_val, y_prob)\n",
    "\n",
    "        print(f\"[{name}] acc={acc:.4f}, AUC={auc:.4f}, n_feats={len(cols)}\")\n",
    "\n",
    "        results.append({\n",
    "            \"subset\": name,\n",
    "            \"n_features\": len(cols),\n",
    "            \"accuracy\": acc,\n",
    "            \"auc\": auc,\n",
    "        })\n",
    "\n",
    "    results_df = pd.DataFrame(results).sort_values(\n",
    "        [\"accuracy\", \"auc\"], ascending=False\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    print(\"\\n=== Summary (sorted by accuracy, then AUC) ===\")\n",
    "    display(results_df)\n",
    "\n",
    "    return results_df\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5. Main: load data, build features, split, run ablations\n",
    "# ============================================================\n",
    "\n",
    "# ---- adjust path to your train.jsonl ----\n",
    "train_path = \"../../train.jsonl\"   # <-- change if needed\n",
    "\n",
    "# 1) parse with author_pseudo_id + raw meta\n",
    "raw_df = parse_tweets_meta(train_path, expect_label=True)\n",
    "\n",
    "# 2) author-based split BEFORE computing stats (important!)\n",
    "train_raw, val_raw = author_based_split(raw_df, val_size=0.1, random_state=42)\n",
    "\n",
    "# 3) build metadata features, with p99 stats fitted on TRAIN only\n",
    "train_meta, stats, src2idx = build_meta_features(train_raw, fit_stats=None, src2idx=None, K=15)\n",
    "val_meta, _, _             = build_meta_features(val_raw, fit_stats=stats, src2idx=src2idx, K=15)\n",
    "\n",
    "# 4) sanity check: label distribution\n",
    "print(\"\\nTrain label distribution:\")\n",
    "print(train_meta[\"label\"].value_counts(normalize=True))\n",
    "print(\"\\nVal label distribution:\")\n",
    "print(val_meta[\"label\"].value_counts(normalize=True))\n",
    "\n",
    "# 5) run ablations\n",
    "results_df = run_metadata_ablations(train_meta, val_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09ddc761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-level split:\n",
      "  #train tweets: 139426\n",
      "  #val tweets:   15488\n",
      "  #unique users train: 44158\n",
      "  #unique users val:   4907\n",
      "\n",
      "Train label distribution:\n",
      "label\n",
      "0    0.534556\n",
      "1    0.465444\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Val label distribution:\n",
      "label\n",
      "0    0.525762\n",
      "1    0.474238\n",
      "Name: proportion, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CamembertModel were not initialized from the model checkpoint at cmarkea/distilcamembert-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Training with BACKBONE FROZEN (head only).\n",
      "[TextOnly][Epoch 0] train_loss=0.6457, val_acc=0.6581, val_auc=0.7135\n",
      "[TextOnly][Epoch 1] train_loss=0.6270, val_acc=0.6607, val_auc=0.7233\n",
      "[TextOnly][Epoch 2] train_loss=0.6222, val_acc=0.6650, val_auc=0.7269\n",
      "\n",
      "Best TextOnly val_acc=0.6650, val_auc=0.7269\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModel, CamembertTokenizer\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "\n",
    "class InfluencerTextOnly(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_model: str = \"cmarkea/distilcamembert-base\",\n",
    "        head_hidden_dim: int = 256,\n",
    "        head_dropout: float = 0.15,\n",
    "        max_len: int = 128,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.tok = CamembertTokenizer.from_pretrained(base_model)\n",
    "        self.enc = AutoModel.from_pretrained(base_model)\n",
    "        dim = self.enc.config.hidden_size\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Dropout(head_dropout),\n",
    "            nn.Linear(dim, head_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(head_dropout),\n",
    "            nn.Linear(head_hidden_dim, 2),\n",
    "            nn.LogSoftmax(dim=1),\n",
    "        )\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def _dev(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    def forward(self, batch):\n",
    "        tok = self.tok(\n",
    "            batch[\"full_text\"],          # list of strings\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(self._dev())\n",
    "        out = self.enc(**tok).last_hidden_state[:, 0]  # CLS / first token\n",
    "        return self.head(out)                          # log-probs (N, 2)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6. Dataset + DataLoaders (reusing parse_tweets_meta + split)\n",
    "# ============================================================\n",
    "\n",
    "class TweetTextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Simple dataset wrapping a DataFrame that has:\n",
    "      - 'full_text' column (already built by parse_tweets_meta)\n",
    "      - 'label' column\n",
    "    \"\"\"\n",
    "    def __init__(self, df):\n",
    "        self.texts = df[\"full_text\"].astype(str).tolist()\n",
    "        self.labels = df[\"label\"].astype(int).tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"full_text\": self.texts[idx],\n",
    "            \"label\": self.labels[idx],\n",
    "        }\n",
    "\n",
    "\n",
    "def text_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate so that:\n",
    "      - model gets a list of strings under batch[\"full_text\"]\n",
    "      - labels become a LongTensor\n",
    "    \"\"\"\n",
    "    texts = [b[\"full_text\"] for b in batch]\n",
    "    labels = torch.tensor([b[\"label\"] for b in batch], dtype=torch.long)\n",
    "    return {\"full_text\": texts, \"label\": labels}\n",
    "\n",
    "\n",
    "def make_text_dataloaders(train_df, val_df, batch_size=16):\n",
    "    train_ds = TweetTextDataset(train_df)\n",
    "    val_ds   = TweetTextDataset(val_df)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds, batch_size=batch_size, shuffle=True,\n",
    "        num_workers=0, collate_fn=text_collate_fn\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_ds, batch_size=batch_size, shuffle=False,\n",
    "        num_workers=0, collate_fn=text_collate_fn\n",
    "    )\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 7. Train / eval loops for InfluencerTextOnly\n",
    "# ============================================================\n",
    "\n",
    "def evaluate_text_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_logits = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            labels = batch[\"label\"].to(device)\n",
    "            logits_logp = model(batch)          # (N, 2) log-probs\n",
    "            logits = logits_logp                # already log-softmax output\n",
    "\n",
    "            all_labels.append(labels.cpu())\n",
    "            all_logits.append(logits.cpu())\n",
    "\n",
    "    all_labels = torch.cat(all_labels).numpy()\n",
    "    all_logits = torch.cat(all_logits, dim=0).numpy()    # log-probs\n",
    "\n",
    "    # predicted class\n",
    "    y_pred = all_logits.argmax(axis=1)\n",
    "    # proba for class 1 (exp of log-prob)\n",
    "    y_prob = np.exp(all_logits)[:, 1]\n",
    "\n",
    "    acc = accuracy_score(all_labels, y_pred)\n",
    "    auc = roc_auc_score(all_labels, y_prob)\n",
    "\n",
    "    return acc, auc\n",
    "\n",
    "\n",
    "def train_text_only_model(\n",
    "    train_df,\n",
    "    val_df,\n",
    "    base_model: str = \"cmarkea/distilcamembert-base\",\n",
    "    max_len: int = 128,\n",
    "    head_hidden_dim: int = 256,\n",
    "    head_dropout: float = 0.15,\n",
    "    batch_size: int = 16,\n",
    "    epochs: int = 3,\n",
    "    # LRs\n",
    "    head_lr: float = 2e-5,\n",
    "    backbone_lr: float = 1e-5,\n",
    "    weight_decay: float = 0.01,\n",
    "    freeze_backbone: bool = True,\n",
    "    device: str = None,\n",
    "):\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    device = torch.device(device)\n",
    "\n",
    "    # DataLoaders (reusing author-based split output)\n",
    "    train_loader, val_loader = make_text_dataloaders(train_df, val_df, batch_size=batch_size)\n",
    "\n",
    "    # Model\n",
    "    model = InfluencerTextOnly(\n",
    "        base_model=base_model,\n",
    "        head_hidden_dim=head_hidden_dim,\n",
    "        head_dropout=head_dropout,\n",
    "        max_len=max_len,\n",
    "    ).to(device)\n",
    "\n",
    "    # -----------------------------\n",
    "    #  Optimizer + freezing logic\n",
    "    # -----------------------------\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    if freeze_backbone:\n",
    "        # Freeze encoder, only train head\n",
    "        for p in model.enc.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            model.head.parameters(),\n",
    "            lr=head_lr,\n",
    "            weight_decay=weight_decay,\n",
    "        )\n",
    "        print(\">> Training with BACKBONE FROZEN (head only).\")\n",
    "    else:\n",
    "        # Train both encoder and head with different LRs\n",
    "        for p in model.enc.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "        param_groups = [\n",
    "            {\"params\": model.enc.parameters(),  \"lr\": backbone_lr, \"weight_decay\": weight_decay},\n",
    "            {\"params\": model.head.parameters(), \"lr\": head_lr,     \"weight_decay\": weight_decay},\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(param_groups)\n",
    "        print(\">> Training with BACKBONE UNFROZEN (separate LRs for backbone/head).\")\n",
    "\n",
    "    best_val_auc = 0.0\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        n_samples = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            labels = batch[\"label\"].to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            logp = model(batch)                 # (N, 2) log-probs\n",
    "            loss = criterion(logp, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            bs = labels.size(0)\n",
    "            running_loss += loss.item() * bs\n",
    "            n_samples += bs\n",
    "\n",
    "        train_loss = running_loss / max(1, n_samples)\n",
    "        val_acc, val_auc = evaluate_text_model(model, val_loader, device)\n",
    "\n",
    "        print(\n",
    "            f\"[TextOnly][Epoch {epoch}] \"\n",
    "            f\"train_loss={train_loss:.4f}, \"\n",
    "            f\"val_acc={val_acc:.4f}, val_auc={val_auc:.4f}\"\n",
    "        )\n",
    "\n",
    "        if val_auc > best_val_auc:\n",
    "            best_val_auc = val_auc\n",
    "            best_val_acc = val_acc\n",
    "\n",
    "    print(f\"\\nBest TextOnly val_acc={best_val_acc:.4f}, val_auc={best_val_auc:.4f}\")\n",
    "    return model, {\"best_val_acc\": best_val_acc, \"best_val_auc\": best_val_auc}\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 8. End-to-end: reuse parse_tweets_meta + author_based_split\n",
    "# ============================================================\n",
    "\n",
    "def run_text_only_experiment(\n",
    "    train_path=\"../../train.jsonl\",\n",
    "    val_size=0.1,\n",
    "    random_state=42,\n",
    "    **train_kwargs,\n",
    "):\n",
    "    \"\"\"\n",
    "    End-to-end run:\n",
    "      - reuse parse_tweets_meta to build full_text + label + author_pseudo_id\n",
    "      - reuse author_based_split for user-level split\n",
    "      - train + evaluate InfluencerTextOnly\n",
    "    \"\"\"\n",
    "    raw_df = parse_tweets_meta(train_path, expect_label=True)\n",
    "    train_raw, val_raw = author_based_split(raw_df, val_size=val_size, random_state=random_state)\n",
    "\n",
    "    print(\"\\nTrain label distribution:\")\n",
    "    print(train_raw[\"label\"].value_counts(normalize=True))\n",
    "    print(\"\\nVal label distribution:\")\n",
    "    print(val_raw[\"label\"].value_counts(normalize=True))\n",
    "\n",
    "    model, scores = train_text_only_model(train_raw, val_raw, **train_kwargs)\n",
    "    return model, scores\n",
    "\n",
    "\n",
    "# Example usage (if you want a direct script entry):\n",
    "if __name__ == \"__main__\":\n",
    "    train_path = \"../../train.jsonl\"\n",
    "    model, scores = run_text_only_experiment(\n",
    "        train_path=\"../../train.jsonl\",\n",
    "        val_size=0.1,\n",
    "        random_state=42,\n",
    "        epochs=3,\n",
    "        batch_size=16,\n",
    "        head_lr=2e-5,\n",
    "        freeze_backbone=True,   # <- default, but explicit\n",
    "    )\n",
    "\n",
    "    # model, scores = run_text_only_experiment(\n",
    "    #     train_path=\"../../train.jsonl\",\n",
    "    #     val_size=0.1,\n",
    "    #     random_state=42,\n",
    "    #     epochs=3,\n",
    "    #     batch_size=16,\n",
    "    #     freeze_backbone=False,\n",
    "    #     head_lr=2e-5,\n",
    "    #     backbone_lr=1e-5,\n",
    "    # )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
